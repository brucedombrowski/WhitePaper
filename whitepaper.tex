\documentclass[11pt, letterpaper, twocolumn]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=1in]{geometry}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{url}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes.geometric, fit, calc}
% Page numbers
\pagestyle{plain}

% Hyperref configuration
\hypersetup{
    colorlinks=true,
    linkcolor=blue!70!black,
    citecolor=blue!70!black,
    urlcolor=blue!70!black,
}

% Code listing style
\lstdefinestyle{powershell}{
    backgroundcolor=\color{gray!10},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=single,
    rulecolor=\color{gray!40},
    numbers=left,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue!70!black}\bfseries,
    commentstyle=\color{green!50!black}\itshape,
    stringstyle=\color{red!70!black},
    showstringspaces=false,
    tabsize=4,
    captionpos=b,
}

\lstdefinestyle{json}{
    backgroundcolor=\color{gray!10},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=single,
    rulecolor=\color{gray!40},
    numbers=left,
    numberstyle=\tiny\color{gray},
    showstringspaces=false,
    tabsize=2,
    captionpos=b,
}

\lstset{style=powershell}

% Title
\title{Git and AI Coding Agents for Government Compliance:\\A Methodology for Federal Information Security Requirements}

\author{
    Bruce Dombrowski\\
    \textit{Independent Researcher}\\
    \small GitHub: brucedombrowski
}

\begin{document}

\maketitle

% ============================================================
\begin{abstract}
Government software development demands rigorous compliance with federal standards including NIST Special Publications, FIPS cryptographic requirements, and CUI handling regulations under 32~CFR~Part~2002. These requirements impose significant documentation overhead---formal requirements traceability, decision memoranda, verification matrices, and regulatory cross-references---that traditionally consumes substantial engineering effort. This paper presents a methodology for combining \textit{git version control} and \textit{AI coding agents} to address this burden. Git provides the tamper-evident audit trail, branching workflow, and configuration management that government frameworks require; AI coding agents draft compliance artifacts, generate structured requirements, and produce traceability documentation under human review. Together, they shift the engineer's role from author to reviewer while maintaining the human-in-the-loop oversight that compliance frameworks demand. We present a five-phase methodology---from requirements capture through version-controlled interaction traceability---and evaluate it through three case studies: a CUI email encryption tool, a formal decision documentation system, and a Security Verification Toolkit implementing automated NIST SP 800-53 control verification. Using Claude Code (Anthropic) as the AI agent implementation, a single engineer produced 642~commits, 34,000+ lines of code, and 136~release tags across seven repositories in 26~calendar days---including 7~decision memoranda, 29~formally traced requirements, and automated verification of 14~NIST SP 800-53 controls. The methodology is agent-platform agnostic; the principles apply to any agentic AI tool with file system access and human approval workflows.
\end{abstract}

\medskip
\noindent\textbf{Keywords:} AI coding agents, git, version control, government compliance, NIST, FIPS, CUI, controlled unclassified information, large language models, software engineering, federal information security, configuration management

% ============================================================
\section{Introduction}
\label{sec:introduction}

Regulated software development imposes a dual burden on developers: the software must correctly implement domain-specific standards, and the \textit{process} of building that software must be formally documented. This burden is not unique to any single domain---it applies equally to safety-critical systems (DO-178C, IEC~61508), information-critical systems (HIPAA, SOX), and the federal information security context examined in this paper. A tool that encrypts files per FIPS~197 \cite{fips197} is insufficient if the development team cannot produce a requirements traceability matrix linking each implementation decision to the governing standard. This documentation overhead---decision memoranda, verification documents, requirements specifications---is where many small teams and independent developers struggle to meet government expectations.

Two technologies converge to address this gap. First, \textit{git version control} provides the tamper-evident, cryptographically hashed change history that government configuration management standards (NIST SP 800-53 CM-3) require. Every change is attributed, timestamped, and linked to its parent state; the commit log serves as a permanent audit record that cannot be silently altered. Second, \textit{AI coding agents}---large language models that operate directly within the developer's file system and terminal, reading source files, generating artifacts, and executing commands under human approval---can draft compliance documents, suggest standard references, and produce structured artifacts at a pace that manual authoring cannot match. However, government work demands accuracy: an incorrect citation to a NIST Special Publication or a mischaracterized FIPS requirement could undermine an entire compliance package.

The combination of git and AI coding agents creates a workflow where the AI drafts and the human reviews, with every interaction captured in a version-controlled, auditable record. This paper demonstrates this approach using Claude Code (Anthropic) as the AI agent implementation, though the methodology applies to any agentic AI tool with file system access and a human-in-the-loop approval model.

This paper makes the following contributions:

\begin{enumerate}[leftmargin=*]
    \item A five-phase methodology for using AI agents in government compliance software development, from requirements capture through version-controlled interaction traceability.
    \item Three case studies demonstrating AI-assisted development of compliance artifacts: SendCUIEmail (a CUI encryption tool), a LaTeX-based decision memoranda system, and a Security Verification Toolkit implementing automated NIST SP 800-53 control verification.
    \item An audit traceability framework using git (configuration management) and GitHub issues (interaction logging) to provide bidirectional provenance between human directives and AI-generated artifacts.
    \item A standards-based review process mapped to IEEE~1028, NIST SP 800-53, and ISO/IEC~25010, with enforced separation of duties between authoring and auditing agents.
    \item A multi-agent architecture for compliance projects, with role-based separation of duties and quantitative output analysis.
\end{enumerate}

% ============================================================
\section{Background and Related Work}
\label{sec:background}

\subsection{Government Compliance Landscape}

Federal information security is governed by a layered framework of executive orders, regulations, and technical standards. Executive Order~13556 established the Controlled Unclassified Information (CUI) program, implemented through 32~CFR~Part~2002 \cite{32cfr2002}. The National Institute of Standards and Technology (NIST) provides the technical backbone through publications including:

\begin{itemize}[leftmargin=*]
    \item \textbf{NIST SP 800-171} \cite{nist800171}: Protecting CUI in Nonfederal Information Systems
    \item \textbf{NIST SP 800-53} \cite{nist80053}: Security and Privacy Controls for Information Systems
    \item \textbf{NIST SP 800-132} \cite{nist800132}: Recommendation for Password-Based Key Derivation
    \item \textbf{FIPS 197} \cite{fips197}: Advanced Encryption Standard (AES)
    \item \textbf{FIPS 140-2} \cite{fips1402}: Security Requirements for Cryptographic Modules
\end{itemize}

Compliance requires not only that software implementations adhere to these standards, but that organizations maintain documentation demonstrating adherence---what auditors term ``evidence of compliance.'' This evidence typically includes requirements specifications, design decisions, test plans, and verification matrices that trace each requirement to its implementation and test.

\subsection{AI-Assisted Software Development}

The application of large language models to software engineering has been studied extensively \cite{fan2023llmse}. Code generation tools such as GitHub Copilot, Amazon CodeWhisperer, and Anthropic's Claude have demonstrated capability in producing syntactically correct code across multiple languages. However, the application of LLMs to \textit{compliance-oriented} development---where correctness encompasses not just functional behavior but regulatory adherence---remains underexplored.

Prior work on AI-assisted documentation generation has focused primarily on API documentation \cite{khan2022apidoc} and code comments. The generation of \textit{regulatory} documentation---where the AI must reason about the relationship between code implementations and published standards---presents distinct challenges including citation accuracy, regulatory interpretation, and the need for conservative (rather than creative) text generation.

The current generation of AI coding tools spans a spectrum of integration depth. \textit{Inline completion} tools (GitHub Copilot, Amazon CodeWhisperer, Tabnine) operate within the editor, suggesting code as the developer types. These tools excel at reducing keystroke-level effort but lack the broader project context needed for compliance work---they cannot read a NIST standard reference and produce a corresponding requirements document. \textit{Chat-based} tools (ChatGPT, Gemini) provide conversational interfaces but operate in isolation from the developer's file system, requiring manual copy-paste of code and artifacts. \textit{Agentic} tools (Claude Code, Cursor, Windsurf, Aider) operate directly within the developer's environment, reading and writing files, executing commands, and maintaining session context. This agentic architecture is essential for compliance work, where the AI must simultaneously reason about source code, published standards, and the traceability relationships between them.

The critical property for compliance applications is an \textit{explicit approval model}: every file write, command execution, and code edit requires human confirmation. While this introduces friction compared to fully autonomous agents, it produces a natural audit trail of human-approved actions---precisely the evidence of human oversight that government compliance frameworks (NIST SP 800-53 AC-5, SA-11) require. Tool permission systems also enable the separation-of-duties pattern described in Section~\ref{sec:agents}, where review agents are denied write access at the tool level rather than by convention. Claude Code implements this model; other agentic tools vary in the granularity of their approval workflows.

\subsection{Agentic AI Tool Architecture}

AI coding agents---exemplified in this paper by Claude Code \cite{claudecode}---operate as command-line agents with access to the developer's local environment. The architectural properties essential for compliance work include:

\begin{enumerate}[leftmargin=*]
    \item \textbf{File system access}: The agent reads and writes files directly, enabling it to analyze source code and produce artifacts in-place.
    \item \textbf{Tool use with approval}: Each action (file read, edit, command execution) requires developer approval, providing the human oversight that compliance frameworks demand.
    \item \textbf{Context persistence}: The agent maintains conversation context across a session, allowing iterative refinement of compliance artifacts.
    \item \textbf{CLAUDE.md conventions}: Projects can include instruction files that persist agent context across sessions, encoding project-specific compliance requirements.
    \item \textbf{Multi-agent mode}: The \texttt{--agents} flag enables orchestrated workflows where specialized agents handle distinct aspects of a project.
    \item \textbf{Session continuity}: The \texttt{--resume} and \texttt{--continue} flags allow sessions to persist across interruptions, preserving the accumulated compliance context that would otherwise need to be reconstructed.
\end{enumerate}

Table~\ref{tab:cli-switches} documents the CLI switches most relevant to compliance workflows. These are recorded in the project's \texttt{CLAUDE.md} to ensure reproducible invocation across sessions and team members.

\begin{table}[htbp]
\centering
\caption{Recommended CLI switches for compliance projects (Claude Code implementation)}
\label{tab:cli-switches}
\begin{tabularx}{\columnwidth}{lX}
\toprule
\textbf{Switch} & \textbf{Purpose} \\
\midrule
\texttt{--agents} & Load multi-agent config (JSON) \\
\texttt{--model} & Select model (opus for review, sonnet for implementation) \\
\texttt{--allowedTools} & Restrict tools per agent role \\
\texttt{--continue} & Resume most recent session \\
\texttt{--verbose} & Log tool calls for audit trail \\
\bottomrule
\end{tabularx}
\end{table}

% ============================================================
\section{Methodology}
\label{sec:methodology}

We developed a methodology for AI-assisted government compliance development organized around five phases, illustrated in Figure~\ref{fig:methodology}. Each phase leverages AI coding agent capabilities while maintaining the human-in-the-loop oversight essential to compliance work.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    phase/.style={
        rectangle, rounded corners=3pt, draw=blue!70!black,
        fill=blue!8, text width=1.9cm, minimum height=0.9cm,
        align=center, font=\footnotesize\bfseries
    },
    artifact/.style={
        rectangle, draw=gray!60, fill=gray!8,
        text width=1.7cm, align=center,
        font=\scriptsize, minimum height=0.6cm
    },
    arr/.style={-{Stealth[length=2mm]}, thick, blue!70!black},
    labelstyle/.style={font=\scriptsize\itshape, text=gray!70!black}
]
% Phases in a flow
\node[phase] (p1) {1. Requirements\\Capture};
\node[phase, right=0.35cm of p1] (p2) {2. Implemen-\\tation};
\node[phase, right=0.35cm of p2] (p3) {3. Decision\\Docs};
\node[phase, below=1.0cm of p1] (p4) {4. Verifi-\\cation};
\node[phase, right=0.35cm of p4] (p5) {5. Version\\Control};

% Arrows between phases
\draw[arr] (p1) -- (p2);
\draw[arr] (p2) -- (p3);
\draw[arr] (p3) -- (p4);
\draw[arr] (p4) -- (p5);

% Artifacts
\node[artifact, below=0.3cm of p1] (a1) {REQ JSON};
\node[artifact, below=0.3cm of p2] (a2) {Source code};
\node[artifact, below=0.3cm of p3] (a3) {DM LaTeX};
\node[artifact, below=0.3cm of p4] (a4) {VER matrix};
\node[artifact, right=0.35cm of p5] (a5) {git + issues};

% Human review bar
\node[rectangle, draw=red!60!black, fill=red!5,
      text width=6.8cm, align=center, font=\scriptsize\bfseries,
      minimum height=0.45cm, below=1.0cm of $(p4)!0.5!(p5)$]
      (review) {Human Review at Every Phase};

\end{tikzpicture}
\caption{Five-phase methodology for AI-assisted compliance development. Human review occurs at every phase transition.}
\label{fig:methodology}
\end{figure}

\subsection{Phase 1: Requirements Capture}

Government projects begin with requirements derived from applicable standards. In our methodology, the developer identifies the governing standards (e.g., NIST SP 800-132 for key derivation) and instructs the AI agent to generate a structured requirements document.

The agent produces requirements in machine-readable JSON format, enabling downstream tooling to generate formatted documents and traceability matrices. Each requirement includes:

\begin{itemize}[leftmargin=*]
    \item A unique identifier (e.g., \texttt{REQ-1.1})
    \item The governing standard and section reference
    \item The requirement text
    \item Classification as mandatory or recommended
    \item Verification method (inspection, test, analysis)
\end{itemize}

Listing~\ref{lst:req-json} shows an excerpt from the SendCUIEmail requirements document, generated with AI agent assistance and reviewed by the developer.

\begin{lstlisting}[style=json, caption={Requirements specification excerpt (REQ-2026-001)}, label={lst:req-json}]
{
  "id": "REQ-1.1",
  "standard": "FIPS 197",
  "section": "Section 1",
  "text": "The tool SHALL use the Advanced
    Encryption Standard (AES) algorithm
    for all file encryption operations.",
  "priority": "mandatory",
  "verification": "inspection"
}
\end{lstlisting}

Requirement text uses RFC~2119 \cite{rfc2119} keywords (SHALL, SHOULD, MAY) to distinguish mandatory from recommended requirements, following the convention established in IETF and NIST publications. The developer's role shifts from \textit{authoring} requirements to \textit{reviewing} them---verifying that the AI's interpretation of the standard is correct and that no requirements are omitted. This review-centric workflow is faster than drafting from scratch while preserving the technical judgment that compliance demands.

\subsection{Phase 2: Implementation with Compliance Awareness}

During implementation, the AI agent operates within the project's instruction files, which encode compliance standards and architectural constraints. \texttt{CLAUDE.md} provides project-wide instructions (build commands, repository scope, conventions), while \texttt{AGENTS.md} defines role-specific compliance context (applicable standards, verification methods, regulatory constraints). Both files persist across sessions, ensuring that every agent invocation begins with the correct compliance posture. Listing~\ref{lst:agentsmd} shows the compliance context from the SendCUIEmail project:

\begin{lstlisting}[style=powershell, caption={AGENTS.md compliance context excerpt}, label={lst:agentsmd}]
## Compliance Standards

- **FIPS 140-2**: AES-256-CBC encryption
- **NIST SP 800-132**: PBKDF2-HMAC-SHA256
    key derivation (100,000 iterations)
- **NIST SP 800-171**: CUI handling
- **32 CFR Part 2002**: CUI marking
\end{lstlisting}

This ensures that every agent session begins with awareness of the applicable standards, reducing the risk of non-compliant suggestions.

\subsection{Phase 3: Decision Documentation}

Government compliance frequently requires documenting \textit{why} a particular approach was chosen, not merely \textit{what} was implemented. Decision memoranda serve this purpose. In our methodology, when the developer makes a design choice (e.g., selecting Cinzel over Trajan Bold for CUI headers, or choosing TikZ over PDF manipulation for form layout), they instruct the agent to generate a formal decision memo.

The LaTeX/Decisions repository implements a template-wrapper pattern where each decision memo defines metadata variables and content, then includes a shared template. Listing~\ref{lst:dm-pattern} illustrates this separation:

\begin{lstlisting}[style=powershell, caption={Decision memo template pattern}, label={lst:dm-pattern}]
\newcommand{\UniqueID}{DM-2026-002}
\newcommand{\DocumentDate}{January 19, 2026}
\newcommand{\AuthorName}{PDF Tools Working Group}
\newcommand{\SubjectField}{Font Selection for
    CUI Header Text}
\newcommand{\dmContent}{...}
\input{_template.tex}
\end{lstlisting}

This pattern enables the AI agent to produce new decision memos by following the established template, ensuring consistency across the documentation package.

\subsection{Phase 4: Verification}

The verification phase produces documents that map each requirement to its implementation evidence. The agent reads the source code, locates the relevant implementation for each requirement, and generates a verification matrix with file paths, line numbers, and explanatory text.

Table~\ref{tab:verification-excerpt} shows an excerpt from the SendCUIEmail verification document.

\begin{table}[htbp]
\centering
\caption{Verification matrix excerpt (VER-2026-001)}
\label{tab:verification-excerpt}
\begin{tabularx}{\columnwidth}{lXl}
\toprule
\textbf{Req.} & \textbf{Evidence} & \textbf{Method} \\
\midrule
REQ-1.1 & \texttt{Encrypt.ps1}: \texttt{[Aes]::Create()} call & Inspection \\
REQ-1.2 & \texttt{\$KEY\_SIZE = 32} (256 bits) & Inspection \\
REQ-2.3 & \texttt{\$ITERATIONS = 100000} & Inspection \\
REQ-3.1 & {\small\texttt{RandomNumberGenerator}} usage & Inspection \\
\bottomrule
\end{tabularx}
\end{table}

\subsection{Phase 5: Version Control and Interaction Traceability}
\label{sec:methodology-git}

The preceding development phases produce artifacts, but compliance also demands \textit{evidence of process}---a verifiable record of who made which decisions, when changes were introduced, and how human-agent interactions shaped the final deliverables. We use git version control and GitHub issues as complementary traceability mechanisms.

\subsubsection{Git as Audit Trail}

Every meaningful action---creating a requirements document, fixing a review finding, adding a compliance scan---is captured as an atomic git commit on the project's main branch. Each commit message describes the compliance-relevant change (e.g., ``Fix all 13 review findings from issue~\#1; add QA standards framework''). This produces a linear, tamper-evident history that auditors can inspect with standard tooling (\texttt{git log}, \texttt{git diff}).

Git's properties align directly with government configuration management requirements. NIST SP 800-53 CM-3 (Configuration Change Control) requires organizations to ``document, approve, and track changes to the system'' \cite{nist80053}. The git commit log serves as this change record: each commit is cryptographically hashed, timestamped, attributed to an author, and linked to its parent commits. Unlike informal change logs, git history cannot be silently altered without breaking the hash chain.

The project uses Semantic Versioning (SemVer) with a \texttt{CHANGELOG.md} following the Keep a Changelog convention. Version numbers encode the significance of changes: major versions for structural reorganization, minor versions for new content, and patch versions for corrections. Each release is tagged (\texttt{git tag~-a~vX.Y.Z}) and the changelog entries reference the GitHub issues that motivated each change. This provides a human-readable change history that complements the machine-level detail in the git log.

The Security Verification Toolkit case study (Section~\ref{sec:casestudy-toolkit}) embeds the git commit hash directly into its compliance attestation PDFs, binding each attestation to a specific, reproducible configuration state.

\subsubsection{GitHub Issues as Interaction Log}

While git captures \textit{what changed}, GitHub issues capture \textit{why it changed} and \textit{who directed the change}. All human-agent interactions in this project are logged as GitHub issues using a structured labeling scheme:

\begin{itemize}[leftmargin=*]
    \item \texttt{human-prompt}: A human directive to the AI agent (e.g., ``expand agents.json with additional agent roles'')
    \item \texttt{agent-output}: Agent-generated analysis or findings (e.g., ``13 review findings per IEEE 1028 inspection'')
    \item \texttt{decision}: A design or process decision with rationale (e.g., ``IT security standards are standard review criteria'')
\end{itemize}

This labeling scheme creates a queryable audit record. An auditor can filter by \texttt{human-prompt} to see every directive the human issued, by \texttt{agent-output} to see every AI-generated analysis, or by \texttt{decision} to trace the rationale for each design choice. The combination provides bidirectional traceability between human intent and AI action---a key requirement when demonstrating human-in-the-loop oversight to government auditors.

All five agents in the multi-agent configuration (Section~\ref{sec:agents}) include interaction logging in their system prompts, requiring them to create GitHub issues for every substantive human-agent exchange. This ensures that the audit trail is comprehensive regardless of which agent is active.

% ============================================================
\section{Case Study: SendCUIEmail}
\label{sec:casestudy-sendcui}

\subsection{Project Overview}

SendCUIEmail \cite{sendcuiemail} is a PowerShell-based tool for encrypting files before email transmission, under active development (currently v0.17.3, pre-release). It is designed for environments where Public Key Infrastructure (PKI) or S/MIME certificate exchange is impractical. The tool addresses a common gap in federal and contractor environments: the need to transmit CUI securely when the only available channel is unencrypted email.

The project's compliance scope spans six federal standards and regulations:

\begin{enumerate}[leftmargin=*]
    \item \textbf{FIPS 197} \cite{fips197}: AES algorithm specification
    \item \textbf{FIPS 140-2} \cite{fips1402}: Cryptographic module validation
    \item \textbf{NIST SP 800-132} \cite{nist800132}: Password-based key derivation
    \item \textbf{NIST SP 800-38A} \cite{nist80038a}: Block cipher modes of operation
    \item \textbf{NIST SP 800-90A} \cite{nist80090a}: Random number generation
    \item \textbf{32 CFR Part 2002} \cite{32cfr2002}: CUI marking and handling
\end{enumerate}

\subsection{AI-Assisted Artifacts}

Over the course of development, the AI agent assisted in producing the following compliance artifacts:

\subsubsection{Requirements Document (REQ-2026-001)}

A JSON-formatted requirements specification containing 29 requirements across six categories: encryption algorithm, key derivation, random number generation, password handling, file format, and platform requirements. The JSON source-of-truth enables automated generation of formatted PDF documents via a Python build script.

\subsubsection{Decision Memoranda (DM-2026-001 through DM-2026-007)}

Seven formal decision memos documenting design choices:

\begin{itemize}[leftmargin=*]
    \item \textbf{DM-001}: Cross-platform support strategy
    \item \textbf{DM-002}: File size limit rationale (10~MB)
    \item \textbf{DM-003}: Password transmission method (out-of-band per NIST SP 800-63B \cite{nist80063b})
    \item \textbf{DM-004}: Verification document numbering scheme
    \item \textbf{DM-005}: Multi-category CUI support per 32~CFR~2002.20(a)(3)
    \item \textbf{DM-006}: Beta readiness assessment
    \item \textbf{DM-007}: Recipient instruction format selection (HTML)
\end{itemize}

\subsubsection{Verification Document (VER-2026-001)}

A line-by-line code verification mapping all 29 requirements to specific implementation evidence in the source code, including file paths, function names, and configuration values.

\subsection{Cryptographic Implementation}

The core encryption implementation demonstrates how AI-assisted development can produce compliant code. The encrypted file format is:

\begin{multline}
\text{Output} = \text{Salt}_{128} \| \text{IV}_{128} \| {} \\
\text{AES-256-CBC}(K, \text{IV}, \text{Plaintext})
\end{multline}

where $K$ is derived via PBKDF2-HMAC-SHA256:

\begin{equation}
K = \text{PBKDF2}(\text{password}, \text{Salt}, 100000, 256)
\end{equation}

The implementation uses exclusively platform-provided cryptographic libraries (\texttt{System.Security.Cryptography}), avoiding third-party dependencies that would complicate FIPS validation. When Windows FIPS mode is enabled, the tool leverages CMVP-validated cryptographic modules (e.g., Certificate \#4515, Kernel Mode Cryptographic Primitives Library, validated under FIPS 140-2 on Windows 10; specific certificate numbers vary by Windows version).

\subsection{Recipient Experience Design}

A significant AI-assisted design contribution was the recipient decryption workflow. The tool generates a self-contained HTML instruction document (\texttt{Decrypt\_Instructions.html}) with an embedded PowerShell one-liner, shown in simplified form in Listing~\ref{lst:oneliner}:

\begin{lstlisting}[style=powershell, caption={Decryption logic (simplified from production code)}, label={lst:oneliner}]
$f=Read-Host "File"
$p=Read-Host "Password"
$d=[IO.File]::ReadAllBytes($f)
$k=[Rfc2898DeriveBytes]::new(
    $p,$d[0..15],100000,"SHA256")
$a=[Aes]::Create()
$a.Key=$k.GetBytes(32)
$a.IV=$d[16..31]
$c=$a.CreateDecryptor()
    .TransformFinalBlock($d,32,$d.Length-32)
[IO.File]::WriteAllBytes(
    ($f-replace'\.Locked$',''),$c)
\end{lstlisting}

This design requires no software installation by recipients---only PowerShell, which is built into every modern Windows installation. The production code uses \texttt{SecureString} with \texttt{SecureStringToBSTR} conversion and file picker dialogs; the listing above is a functionally correct simplification using plaintext password input for clarity. The AI agent helped iterate on the production one-liner to minimize its length while maintaining compliance with the cryptographic parameter requirements.

% ============================================================
\section{Case Study: Decision Documentation System}
\label{sec:casestudy-decisions}

The LaTeX/Decisions repository (v0.4, under active development) demonstrates AI-assisted creation of a reusable documentation system for formal decision memoranda. Government programs frequently require Decision Memoranda (DMs) to document technical and policy choices with traceable rationale.

\subsection{Template Architecture}

The system uses a template-wrapper pattern where a shared base template (\texttt{\_template.tex}) defines the document layout---headers with organizational logo, footers with document ID and page numbering, and standardized section formatting---while individual decision documents supply metadata and content through LaTeX command definitions.

This separation of concerns enables AI agents to produce new decision memos by populating the established template structure, ensuring visual and structural consistency without requiring the agent to understand the full LaTeX layout implementation.

\subsection{SF901 CUI Coversheet Compliance}

Three decision memos (DM-2026-001 through DM-2026-003) document the technical approach to generating Standard Form~901 CUI coversheets:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Implementation approach}: LaTeX template recreation rather than PDF manipulation, chosen for alignment with existing infrastructure and independence from external tools.
    \item \textbf{Font selection}: Cinzel (open-source, SIL OFL) chosen over Trajan Bold (commercial) for the CUI header, balancing visual fidelity with licensing constraints.
    \item \textbf{Layout strategy}: TikZ with absolute positioning for pixel-precise form reproduction, justified by the form's stability (unchanged since November 2018 per GSA records).
\end{enumerate}

Each decision memo follows the format required by many government programs: identification of options considered, evaluation criteria, selected approach, and rationale with regulatory references.

% ============================================================
\section{Case Study: Security Verification Toolkit}
\label{sec:casestudy-toolkit}

The third case study examines the Security Verification Toolkit \cite{securitytoolkit}, a pure-Bash security scanning and compliance documentation system under active development (currently v2.7.3) that automates the verification of federal security controls. Like all three case studies in this paper, the toolkit is under active development and is presented as a case study in AI-assisted compliance tooling, not as a finished product. Unlike SendCUIEmail (which implements a single compliance function) or the Decision Documentation System (which manages process artifacts), the toolkit addresses the \textit{continuous compliance verification} challenge: demonstrating ongoing adherence to NIST SP 800-53 and NIST SP 800-171 controls through automated scanning and attestation generation.

\subsection{Scope and Standards}

The toolkit implements 14 NIST SP 800-53 controls and 11 NIST SP 800-171 controls across eight security control families, with each scan mapped to its governing control in machine-readable JSON. The standards addressed include:

\begin{itemize}[leftmargin=*]
    \item \textbf{NIST SP 800-53} \cite{nist80053}: AU-2/3 (Audit Events and Records), CA-2 (Assessment), CM-6/8 (Configuration), MP-6 (Media Sanitization), RA-5 (Vulnerability Scanning), SA-11 (Developer Testing), SC-8 (Transmission Protection), SI-2/3/4/5/12 (Information Integrity)
    \item \textbf{NIST SP 800-171} \cite{nist800171}: 11 corresponding CUI protection requirements
    \item \textbf{NIST SP 800-88} \cite{nist80088}: Media sanitization (secure deletion)
    \item \textbf{BOD 22-01} \cite{bod2201}: CISA Known Exploited Vulnerabilities cross-referencing
    \item \textbf{FIPS 199} \cite{fips199}: Security categorization of federal information
\end{itemize}

\subsection{Requirements Traceability}

The toolkit maintains a complete traceability chain in JSON format:

\begin{equation*}
\text{Requirement} \rightarrow \text{NIST Control} \rightarrow \text{Script} \rightarrow \text{Test} \rightarrow \text{Evidence}
\end{equation*}

A \texttt{mapping.json} file links 14 functional requirements (FR-001 through FR-014) to NIST controls, implementation scripts, and test cases, navigable in three directions: by script, by NIST 800-53 control, and by NIST 800-171 control. This bidirectional traceability enables auditors to verify compliance from any starting point---a requirement that many government programs mandate but few tools automate.

The AI agent assisted in generating this traceability framework, producing the initial JSON mappings from the NIST control catalog and iterating with the developer to ensure completeness. The machine-readable format enables downstream automation: generating formatted traceability reports, validating that no requirements are orphaned, and detecting when code changes break previously-verified controls.

\subsection{Automated Attestation Generation}

A distinguishing feature of the toolkit is its automated generation of formal compliance attestations as PDFs via LaTeX templates. After each scan run, the system produces timestamped, checksummed attestation documents suitable for inclusion in government compliance packages. Each attestation includes:

\begin{itemize}[leftmargin=*]
    \item Toolkit version and git commit hash (configuration management)
    \item Scan timestamp in ISO 8601 UTC
    \item SHA-256 checksums of all scan outputs
    \item NIST control mappings for each finding
    \item CUI markings where applicable
\end{itemize}

The toolkit's design philosophy---\textit{``You are only as good as your last scan''}---enforces that every scan run overwrites previous results, preventing stale attestations from being presented as current evidence. This maps directly to the continuous monitoring requirements of NIST CA-7.

\subsection{Codebase Structure Evolution}

Code survival analysis using git-of-theseus reveals how the toolkit's codebase structure evolved over its 463-commit history. Figure~\ref{fig:theseus-dirs} shows that the \texttt{scripts/} (implementation) and \texttt{tests/} (verification) directories grew in lockstep throughout the development period---from approximately 10,000 and 3,000 lines respectively at the start of the analyzed period to 27,000 and 8,000 lines at the time of writing. This parallel growth indicates disciplined test coverage practices maintained throughout AI-assisted development: new scanning capabilities were consistently accompanied by corresponding test cases.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{visualizations/theseus_directories.pdf}
\caption{Security Verification Toolkit codebase structure evolution (git-of-theseus). The \texttt{scripts/} and \texttt{tests/} directories grow in lockstep, indicating consistent test coverage practices throughout AI-assisted development.}
\label{fig:theseus-dirs}
\end{figure}

\subsection{Multi-Agent Development}

The toolkit itself was developed using a multi-agent architecture with defined roles: Lead Systems Engineer, QA Engineer, Windows Developer, Documentation Engineer, and Lead Software Developer. Agent coordination uses GitHub issues as the communication channel---the same interaction logging pattern adopted in this paper's methodology. This represents a mature implementation of the multi-agent compliance workflow described in Section~\ref{sec:agents}, validated across 94 version tags and over 140 GitHub issues.

% ============================================================
\section{Multi-Agent Workflow}
\label{sec:agents}

Multi-agent orchestration enables workflows where multiple specialized AI agents collaborate on a project, each with defined roles, permitted tools, and compliance context. For government compliance work, we propose the role-based agent architecture shown in Figure~\ref{fig:agents}.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    agent/.style={
        rectangle, rounded corners=3pt, draw=#1!70!black,
        fill=#1!10, text width=2.0cm, minimum height=0.8cm,
        align=center, font=\footnotesize\bfseries
    },
    model/.style={
        font=\scriptsize, text=#1!70!black
    },
    tools/.style={
        rectangle, draw=gray!50, fill=gray!5,
        font=\scriptsize, align=center, minimum height=0.4cm
    },
    arr/.style={-{Stealth[length=2mm]}, thick, gray!60},
    human/.style={
        ellipse, draw=red!60!black, fill=red!5,
        font=\footnotesize\bfseries, align=center,
        minimum width=2.0cm, minimum height=0.7cm
    }
]

% Human at top
\node[human] (human) {Human\\Engineer};

% Agents row 1
\node[agent=teal, below left=1.0cm and 1.2cm of human] (setup) {Project\\Setup};
\node[agent=blue, below=1.0cm of human] (req) {Require-\\ments};
\node[agent=teal, below right=1.0cm and 1.2cm of human] (impl) {Implemen-\\tation};

% Agents row 2
\node[agent=teal, below=2.4cm of human, xshift=-1.3cm] (doc) {Documen-\\tation};
\node[agent=orange, below=2.4cm of human, xshift=1.3cm] (rev) {Review};

% Model labels
\node[model=teal, below=0pt of setup] {\textit{Sonnet}};
\node[model=blue, below=0pt of req] {\textit{Opus}};
\node[model=teal, below=0pt of impl] {\textit{Sonnet}};
\node[model=teal, below=0pt of doc] {\textit{Sonnet}};
\node[model=orange, below=0pt of rev] {\textit{Opus}};

% No-write indicator
\node[font=\scriptsize\color{red!70!black}, below=9pt of rev] {read-only};

% Arrows from human
\draw[arr] (human) -- (setup);
\draw[arr] (human) -- (req);
\draw[arr] (human) -- (impl);
\draw[arr] (human) -- (doc);
\draw[arr] (human) -- (rev);

% GitHub issues bar
\node[rectangle, draw=black!60, fill=yellow!10,
      text width=5.0cm, align=center,
      font=\scriptsize\bfseries, minimum height=0.4cm,
      below=2.0cm of $(doc)!0.5!(rev)$]
      (gh) {GitHub Issues (audit trail)};

% Dashed lines to GitHub
\draw[dashed, gray!50] (setup.south) -- ++(0,-0.62) -| (gh.north west);
\draw[dashed, gray!50] (rev.south) -- ++(0,-0.17) -| (gh.north east);

\end{tikzpicture}
\caption{Multi-agent architecture for compliance projects. Teal agents use Sonnet; blue/orange agents use Opus. The review agent has read-only access (NIST SP 800-53 AC-5). All agents log interactions to GitHub issues.}
\label{fig:agents}
\end{figure}

\subsection{Agent Roles}

\begin{enumerate}[leftmargin=*]
    \item \textbf{Project Setup Agent}: Initializes repository structure, creates \texttt{CLAUDE.md} with compliance context, establishes documentation templates and directory layout.
    \item \textbf{Requirements Agent}: Analyzes governing standards and generates structured requirements documents in JSON format.
    \item \textbf{Implementation Agent}: Writes compliant code within the constraints defined by the requirements and \texttt{CLAUDE.md} context.
    \item \textbf{Documentation Agent}: Produces decision memoranda, verification documents, and traceability matrices.
    \item \textbf{Review Agent}: Audits artifacts for completeness, citation accuracy, and cross-reference integrity.
\end{enumerate}

\subsection{Agent Configuration}

Agent definitions are stored in a JSON configuration file that specifies each agent's role, model selection, permitted tools, and a detailed system prompt encoding compliance context. Table~\ref{tab:agent-config} summarizes the five-agent configuration developed for this paper.

\begin{table*}[htbp]
\centering
\caption{Agent configuration summary (agents.json)}
\label{tab:agent-config}
\begin{tabularx}{\textwidth}{llllX}
\toprule
\textbf{Agent} & \textbf{Model} & \textbf{Phase} & \textbf{QA Standard} & \textbf{Key Capability} \\
\midrule
project-setup & Sonnet & Setup & --- & Repo structure, build config, templates \\
requirements & Opus & Phase 1 & IEEE 29148 & Standard interpretation, JSON requirements \\
implementation & Sonnet & Phase 2 & --- & Compliant code within REQ constraints \\
documentation & Sonnet & Phases 3--4 & MIL-STD-498 & Decision memos, verification docs, LaTeX \\
review & Opus & Cross-cutting & IEEE 1028, NIST 800-53 AC-5 & Audit with no write access (read-only) \\
\bottomrule
\end{tabularx}
\smallskip
\par\small\textit{Note:} Phase 5 (Version Control) is performed by all agents via GitHub issue logging. The review agent operates across phases rather than within a single phase.
\end{table*}

Model selection reflects the cognitive demands of each role: the \texttt{requirements} and \texttt{review} agents use Opus for its stronger reasoning over regulatory interpretation and cross-reference validation, while \texttt{implementation} and \texttt{documentation} use Sonnet for its favorable speed-to-quality ratio on structured, template-following tasks. Notably, the \texttt{review} agent is denied write and edit tools, enforcing a separation-of-duties principle where auditors identify problems but do not fix them.

\subsection{Workflow Orchestration}

The multi-agent workflow proceeds through the five phases described in Section~\ref{sec:methodology}, with each agent operating within its defined scope. The key advantage of this architecture is \textit{context isolation}: the requirements agent does not need the full implementation context, and the documentation agent can focus on artifact generation without the overhead of the full codebase in its context window.

This isolation is particularly valuable for government projects where compliance documentation can be extensive---a full NIST SP 800-171 assessment may reference over 100 security requirements, and maintaining all of these in a single agent context is impractical. The separation-of-duties between the \texttt{documentation} and \texttt{review} agents also mirrors the organizational controls common in government programs, where the author of a compliance artifact should not be the sole reviewer.

\subsection{Scrum-Based Agent Orchestration}
\label{sec:scrum}

The pipeline model described above---where agents execute sequentially through defined phases---is effective for linear compliance workflows but does not accommodate the iterative, feedback-driven nature of real-world software development. An alternative orchestration model maps AI agents to a Scrum team structure, drawing on the Scrum Guide \cite{scrumguide} framework that is widely adopted in both commercial and government software programs.

In this model, AI agents assume Scrum roles:

\begin{itemize}[leftmargin=*]
    \item \textbf{Product Owner agent}: Maintains the product backlog (GitHub issues), prioritizes work items based on compliance risk and stakeholder value, and defines acceptance criteria. Uses Opus for its judgment over regulatory priorities.
    \item \textbf{Scrum Master agent}: Facilitates sprint execution, identifies impediments (blocked issues, failing scans, unresolved review findings), and ensures the team adheres to process standards. Monitors GitHub issues for stalled work and escalates to the human.
    \item \textbf{Developer agents}: One or more implementation and documentation agents that pull work from the sprint backlog and produce deliverables. Use Sonnet for throughput on structured tasks.
\end{itemize}

This Scrum-based approach offers several advantages for compliance projects:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Iterative refinement}: Rather than producing all requirements before any implementation begins, the team works in sprints where each iteration can incorporate feedback from the previous sprint's review---mirroring the review-centric workflow discussed in Section~\ref{sec:discussion}.
    \item \textbf{Backlog as audit trail}: The GitHub issue backlog serves dual purpose: it is both the Scrum product backlog and the compliance audit trail (Section~\ref{sec:methodology-git}). Each sprint produces a traceable increment of compliance artifacts.
    \item \textbf{Human as stakeholder}: The human engineer acts as the primary stakeholder (and may also serve as the Product Owner), providing direction at sprint boundaries rather than approving every individual action. This scales the human-in-the-loop model to larger projects.
    \item \textbf{Government familiarity}: Scrum and Agile methodologies are already adopted across federal agencies under guidance such as the GSA 18F Agile Practices Guide and the DoD Agile Software Acquisition Guidebook, reducing the organizational friction of adopting AI-assisted workflows.
\end{enumerate}

A reference implementation of this Scrum-based agent architecture is under development at \url{https://github.com/brucedombrowski/Scrum}, applying the Scrum Guide's ceremonies and artifacts to AI agent multi-agent orchestration. This represents a natural evolution from the sequential pipeline model to a more flexible, sprint-based orchestration that better accommodates the iterative nature of compliance development.

\subsection{Ecosystem Architecture}

The multi-agent workflow, Scrum orchestration, and underlying process framework are maintained as a set of interrelated repositories with clean separation of concerns:

\begin{itemize}[leftmargin=*]
    \item \textbf{systems-engineering} (\url{https://github.com/brucedombrowski/systems-engineering}): Defines \textit{how} work is done---the five-phase process, standards framework, traceability model, and artifact conventions.
    \item \textbf{ai-agents} (\url{https://github.com/brucedombrowski/ai-agents}): Defines \textit{who} does the work---model-agnostic agent role templates with vendor-specific implementations.
    \item \textbf{Scrum} (\url{https://github.com/brucedombrowski/Scrum}): Defines \textit{when} work happens---sprint cadence, backlog management, and Scrum ceremonies.
\end{itemize}

Individual project repositories (SendCUIEmail, security-toolkit, this white paper) consume these shared definitions while maintaining project-specific instructions. This architecture allows the process, agent templates, and orchestration model to evolve independently and be adopted incrementally by new projects.

% ============================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Quality of AI-Generated Compliance Artifacts}

Our experience indicates that AI coding agents produce compliance artifacts that are \textit{structurally sound} but require careful human review for \textit{substantive accuracy}. The AI reliably generates:

\begin{itemize}[leftmargin=*]
    \item Correct document structure and formatting
    \item Appropriate standard references (e.g., citing NIST SP 800-132 for PBKDF2)
    \item Reasonable requirement decomposition
    \item Accurate code-to-requirement tracing when given source access
\end{itemize}

Areas requiring human review include:

\begin{itemize}[leftmargin=*]
    \item \textit{Regulatory interpretation}: Whether a requirement is ``mandatory'' vs. ``recommended'' per the governing standard
    \item \textit{Completeness}: Whether all applicable requirements from a standard have been captured
    \item \textit{Citation precision}: Verifying specific section numbers within standards
    \item \textit{Organizational context}: Tailoring requirements to the specific compliance posture of the organization
\end{itemize}

\subsection{Quantitative Output Analysis}

Table~\ref{tab:output-metrics} summarizes the measurable output of the AI-assisted methodology across the three case studies and supporting infrastructure, produced by a single engineer over 26~calendar days.

\begin{table}[htbp]
\centering
\caption{Aggregate output metrics (26-day period, single engineer)}
\label{tab:output-metrics}
\begin{tabularx}{\columnwidth}{Xr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Git repositories & 7 \\
Total commits & 642 \\
Lines of code & 34,016 \\
Release tags & 136 \\
Decision memoranda & 7 \\
Requirements (JSON) & 29 \\
Verification mappings & 29 \\
Security scan scripts & 14 \\
Test scripts & 14 \\
NIST controls automated & 14 \\
GitHub issues (audit trail) & 170+ \\
\bottomrule
\end{tabularx}
\end{table}

The daily commit rate averaged 24.7~commits/day, with the Security Verification Toolkit alone accounting for 463~commits, 94~version tags, and 26,630~lines of code. These figures are not presented as benchmarks---the commit rate reflects a development style characterized by frequent, atomic commits rather than large batch changes---but they indicate the throughput achievable when AI agents handle drafting and the engineer focuses on review and direction.

Figure~\ref{fig:code-churn} shows the daily code churn across all repositories. Every period shows net-positive creation (green additions exceeding red deletions), with a sustained deletion rate indicating active refactoring rather than write-once code. The 48,000-line addition spike (late January) corresponds to the Security Verification Toolkit's test suite expansion---a compliance-critical investment that the AI agent produced concurrently with the scanning scripts.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{visualizations/code_churn.pdf}
\caption{Code churn across all repositories. Green bars show lines added; red bars show lines deleted. Net-positive creation in every period, with sustained deletion indicating active refactoring.}
\label{fig:code-churn}
\end{figure}

More significant than the raw volume is the \textit{ratio of compliance artifacts to implementation code}. Traditional compliance workflows produce documentation as a separate, sequential activity after implementation. In the AI-assisted workflow, compliance artifacts (requirements, verification matrices, decision memos, traceability mappings, attestation PDFs) are generated \textit{concurrently} with implementation as a natural byproduct of the agent interaction. The 7~decision memoranda, 29~requirements, and 29~verification mappings in SendCUIEmail were produced in the same sessions that generated the implementation code, not in a separate documentation sprint.

\subsection{The Review-Centric Workflow}

The most significant shift introduced by AI-assisted compliance development is the transition from an \textit{authoring} model to a \textit{review} model. In traditional compliance work, an engineer reads the governing standard, interprets its requirements, drafts the compliance artifact, and submits it for review. With AI assistance, the engineer specifies the standard and reviews the AI-generated artifact for accuracy.

This shift has two implications. First, it is faster: reviewing a draft is consistently less effort than producing one from scratch. Second, it changes the \textit{skill profile} required: the engineer must be a competent reviewer of compliance documents rather than a competent author. This is a meaningful distinction---many engineers who understand the technical standards struggle with the formal writing conventions of government documentation.

\subsection{Concurrent Multi-Project Scalability}

The review-centric workflow has a second-order implication: because the human's role shifts from author to reviewer, a single engineer can oversee multiple AI-assisted projects concurrently. During the development of this paper, the author maintained five active projects simultaneously---SendCUIEmail (CUI encryption), a decision documentation system, a Security Verification Toolkit, this white paper, and a Scrum-based agent orchestration system---each with its own AI agent sessions and compliance artifacts. These five projects span seven git repositories tracked in this paper's visualization data, with additional supporting repositories (agent templates, process framework) bringing the total to 16.

Critically, these projects are not merely concurrent; they \textit{cross-pollinate}. Patterns discovered in one project feed into others: the Security Verification Toolkit's scanning infrastructure was applied to the white paper repository (Section~\ref{sec:casestudy-toolkit}); the SendCUIEmail project's agent conventions (\texttt{AGENTS.md}) informed the multi-agent architecture described in Section~\ref{sec:agents}; the Scrum repo's team structure informed Section~\ref{sec:scrum}; and this paper documents the methodology used across all projects, creating a feedback loop that improves each project's compliance posture.

Figure~\ref{fig:cumulative-commits} illustrates this concurrent development pattern: the cumulative commit timeline shows multiple repositories advancing simultaneously, with the Security Verification Toolkit exhibiting the steepest growth curve while other projects progress in parallel bursts.

This cross-project learning is facilitated by the \texttt{CLAUDE.md} convention: insights captured in one project's instructions propagate to others when the engineer applies the same patterns (semantic versioning, interaction logging, QA standards) across repositories. The human engineer serves as the integrator---reviewing agent output across projects, recognizing transferable patterns, and directing agents to apply lessons learned from one domain to another. This is a scalability model that would be impractical without AI assistance: the documentation and compliance overhead of five simultaneous government projects would overwhelm a single engineer working manually.

Moreover, the methodology itself is refined iteratively as the projects progress. The interaction logging requirement (Section~\ref{sec:methodology-git}) did not exist at project inception---it was added mid-session when the engineer recognized the need for audit traceability. Similarly, semantic versioning, the \texttt{build.sh} script, and the Scrum-based orchestration model were all incorporated as the engineer observed gaps during active development. This organic refinement is itself documented in the GitHub issue trail, creating a meta-level record of how the compliance process evolved. The ability to refine tooling and process \textit{while simultaneously producing compliant artifacts} is a distinctive advantage of the AI-assisted approach: the agent can update its own instructions, rebuild its infrastructure, and continue producing deliverables without the context-switching penalty that a human author would incur.

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{visualizations/cumulative_commits.pdf}
\caption{Cumulative commits across all seven repositories over the four-week development period. The Security Verification Toolkit (green, dotted) dominates with 463 commits and exhibits the steepest growth curve. Multiple repositories advance concurrently, demonstrating the scalability of the AI-assisted review-centric workflow.}
\label{fig:cumulative-commits}
\end{figure*}

The scope of this concurrent work extends beyond software compliance. The author's AI-assisted workflow originated with a CAD-based house construction project and a speech-processing application before evolving into the government compliance domain examined here. Across 16 active repositories---spanning systems engineering, hardware interfaces, music production, web development, and federal information security---the same patterns apply: AI agents draft artifacts, the human reviews and directs, and the process is documented through git and issue tracking. The methodology is not specific to government compliance; it is a general-purpose approach to engineering documentation that happens to map well to federal requirements.

A key enabler of cross-project learning is \textit{agent instruction ingestion}: AI agents read instruction files (\texttt{CLAUDE.md}, \texttt{AGENTS.md}, \texttt{agents.json}) from other repositories, absorbing patterns, conventions, and compliance requirements established in sibling projects. This naturally led to the creation of a canonical agent templates repository (\url{https://github.com/brucedombrowski/ai-agents}) containing model-agnostic role definitions that any project can inherit. The templates separate the \textit{what} (role responsibilities, standards, interaction protocols) from the \textit{how} (vendor-specific model selection and tool configuration), allowing the same compliance agent patterns to be implemented across different AI platforms. This repository itself emerged organically from the white paper development process---an example of the methodology producing reusable infrastructure as a byproduct of compliance work.

\subsection{Stakeholder Accessibility: Bridging the CLI-Browser Gap}

The methodology described in this paper is CLI-first: the engineer works in AI coding agents, git, and shell scripts. This creates an adoption barrier when the goal is team-wide participation by non-technical stakeholders---program managers, team leads, auditors---who will not install command-line tools.

The key insight is that while the \textit{engineering} happens in the CLI, the \textit{outputs} are entirely browser-accessible. GitHub and GitLab web interfaces render commit history, file diffs (green lines for additions, red for deletions), issue threads, and merge request discussions without requiring any software installation. The stakeholder's workflow reduces to: open a URL, review the diff, leave a comment, click approve. This is a one-page desk instruction, not a training program.

This pattern was validated in practice: a team lead adopted GitLab for versioning periodic database exports to CSV, adding configuration management (NIST SP 800-53 CM-3) to previously untracked operational data. The team lead did not learn git---they learned to click ``History'' and read a diff. The CSV format is critical: unlike binary formats (Excel \texttt{.xlsx}, PDF), CSV files produce human-readable line-by-line diffs in the browser. For teams working with Excel files, a pre-commit hook that auto-converts \texttt{.xlsx} to \texttt{.csv} provides the same visibility without changing the user's workflow.

When whole-team review is required, the branch-and-merge-request workflow provides structured approval entirely within the browser. Branch protection rules enforce that (1) all changes to the main branch must go through a merge request, (2) required reviewers must approve before merge, and (3) the author cannot approve their own changes. These guardrails map directly to NIST SP 800-53 CM-3 (change control), AC-5 (separation of duties), and AU-3 (audit trail). Once configured, they are enforced automatically---the merge button is physically disabled until all conditions are met.

The generated visualizations (Section~\ref{sec:visualization}) serve a similar accessibility function. A chart showing 642 commits across 7 repositories in under four weeks communicates project scope more effectively to a non-technical audience than any paragraph. The animated tree visualization (gource) showing file creation and modification over time has proven particularly effective for conveying the scale and structure of development activity to stakeholders unfamiliar with version control concepts.

\subsection{Git Data Visualization}
\label{sec:visualization}

To support both the research objectives of this paper and the practical need to communicate project status to non-technical stakeholders, we developed a visualization pipeline that extracts data from git repositories and produces publication-quality charts.

The pipeline follows the sequence: \texttt{git log} (structured data extraction) $\rightarrow$ pandas (aggregation and analysis) $\rightarrow$ matplotlib with SciencePlots styling (IEEE-formatted charts) $\rightarrow$ matplot2tikz (PGFPlots export for native LaTeX inclusion). Each chart is generated in three formats: PNG (300~DPI, for presentations), PDF (vector, for print), and TikZ (\texttt{.tex}, for direct \texttt{\textbackslash input\{\}} into LaTeX documents).

The toolkit comprises both custom analysis scripts and established open-source tools:

\begin{itemize}[leftmargin=*]
    \item \textbf{onefetch}: Repository summary cards showing languages, lines of code, commits, and version tags per repo.
    \item \textbf{git-of-theseus}: Code survival analysis using Kaplan-Meier methods---cohort stack plots showing how code ages over time, and extension/directory breakdowns showing how the codebase structure evolves.
    \item \textbf{gource}: Animated tree visualization rendering repository history as a growing organism, with files as nodes and contributors as actors.
    \item \textbf{Custom cross-repo analysis}: Cumulative commit timelines, daily activity by repository, code churn (additions vs. deletions), commit pattern analysis (hour of day, day of week), and ecosystem timeline (Gantt-style active development windows).
\end{itemize}

Applied to the seven repositories in this ecosystem (totaling 642 commits and 34,000+ lines of code over four weeks), the visualizations revealed several patterns. Figure~\ref{fig:repo-comparison} shows the ecosystem overview: the Security Verification Toolkit dominates with 463 commits, 26,630 lines of code, and 94 version tags. Figure~\ref{fig:ecosystem-timeline} shows the active development windows---multiple repositories developed concurrently by a single engineer with AI agent support. Additional findings include: \texttt{scripts/} and \texttt{tests/} directories grew in lockstep (indicating disciplined test coverage); development activity concentrated on weekdays with near-zero weekend commits; and the code cohort analysis confirmed that all code is 2026-vintage---consistent with a rapidly growing project where code survival analysis is not yet meaningful but growth trajectories are clearly visible.

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{visualizations/repo_comparison.pdf}
\caption{Repository ecosystem overview. Left: total commits per repository. Center: lines of code. Right: version tags (releases). The Security Verification Toolkit dominates all three metrics, reflecting its maturity as the most actively developed case study.}
\label{fig:repo-comparison}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{visualizations/ecosystem_timeline.pdf}
\caption{Active development windows for each repository. Bar length indicates the period between first and last commit; labels show total commit counts. Multiple repositories were developed concurrently by a single engineer using AI agent assistance.}
\label{fig:ecosystem-timeline}
\end{figure*}

Figure~\ref{fig:commit-patterns} shows the temporal distribution of commits: peak activity occurs at 17:00~UTC (noon Eastern) and 03:00--04:00~UTC (late night), with near-zero weekend commits. This pattern---high weekday intensity with no weekend work---is characteristic of a sustainable AI-assisted development cadence where the engineer directs intensive sessions during focused work hours rather than spreading effort thinly across calendar time.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{visualizations/commit_patterns.pdf}
\caption{Development patterns across the ecosystem. Left: commits by hour of day (UTC). Right: commits by day of week (blue = weekday, orange = weekend). Peak activity at 17:00 UTC and 03:00--04:00 UTC; near-zero weekend commits.}
\label{fig:commit-patterns}
\end{figure}

These visualizations serve dual purpose: they are research artifacts that quantify the development activity described in this paper, and they are communication tools that make the same data accessible to non-technical reviewers through browser-viewable charts and an animated video. A training slide deck (\texttt{git-workflow-training.pptx}) and a desk instruction (\texttt{DI-GIT-001}) were produced as companion artifacts to support organizational adoption.

\subsection{Human-in-the-Loop Compliance}

Government frameworks increasingly require evidence of human oversight in automated processes. Agentic AI tools with explicit permission models---where each file write, command execution, and code edit requires developer approval---provide natural evidence of human-in-the-loop oversight. Every action taken by the agent is logged and approved, creating an audit trail that maps to the ``authorized use'' requirements common in government security frameworks.

The \texttt{CLAUDE.md} convention further supports compliance by encoding organizational and project-specific constraints that persist across sessions. An organization's compliance officer could define \texttt{CLAUDE.md} templates that encode mandatory requirements, ensuring that all AI-assisted development within the organization operates within approved boundaries.

\subsection{Standards-Based Review Process}

The review agent itself operates according to established QA standards, making the review process auditable and reproducible. Table~\ref{tab:qa-standards} maps each aspect of the review process to its governing standard.

\begin{table*}[htbp]
\centering
\caption{QA standards applied to the AI-assisted review process}
\label{tab:qa-standards}
\begin{tabularx}{\textwidth}{llX}
\toprule
\textbf{Standard} & \textbf{Control} & \textbf{Application} \\
\midrule
IEEE 1028 \cite{ieee1028} & Software Reviews & Review structure: severity classification (CRITICAL/MINOR), findings format, disposition \\
IEEE 29148 \cite{ieee29148} & Requirements Engineering & Traceability verification: standard $\rightarrow$ requirement $\rightarrow$ implementation $\rightarrow$ test \\
NIST SP 800-53 \cite{nist80053} & AC-5: Separation of Duties & Review agent denied write/edit tools; auditors cannot modify what they audit \\
NIST SP 800-53 \cite{nist80053} & SA-11: Developer Testing & Claims verified against source files; assertions checked against implementation \\
ISO/IEC 25010 \cite{iso25010} & Software Quality & Documentation quality: completeness, accuracy, consistency checks \\
MIL-STD-498 \cite{milstd498} & A.5.19: Traceability & Cross-reference integrity between REQ, VER, DM, and source code \\
\bottomrule
\end{tabularx}
\end{table*}

This standards-based approach ensures that the review process itself can withstand audit scrutiny---a critical consideration for government programs where the QA methodology must be as defensible as the artifacts it evaluates. All review findings are documented as GitHub issues with structured severity, recommendation, and standard-violated fields, providing a traceable audit record per IEEE~1028.

\subsection{Limitations}

Several limitations should be noted:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Model knowledge currency}: LLM training data has a cutoff date, meaning recent revisions to standards (e.g., updates to NIST SP 800-171 Rev.~3, or the transition from FIPS 140-2 to FIPS 140-3 for new CMVP submissions since 2021) may not be reflected. Developers must verify that AI-cited standards are current.
    \item \textbf{No formal verification}: AI-generated compliance claims are assertions, not proofs. They do not substitute for formal testing, independent audit, or certification processes such as CMVP validation.
    \item \textbf{Organizational specificity}: Government compliance is highly context-dependent. The same standard may be interpreted differently across agencies, and AI agents lack organizational knowledge without explicit instruction.
    \item \textbf{Classification boundaries}: AI tools operating in cloud-connected modes are unsuitable for classified work. The methodology presented here applies only to unclassified and CUI environments.
    \item \textbf{Case study maturity}: All three case study projects are under active development and have not reached production release status (SendCUIEmail v0.17.3, Decisions v0.4, Security Toolkit v2.7.3). The compliance artifacts and methodology demonstrated here reflect a development-phase workflow; production deployment would require additional validation, independent testing, and formal authority-to-operate processes.
\end{enumerate}

\subsection{Reproducibility and Process Documentation}

This paper itself was produced using the methodology it describes. The white paper repository maintains a two-tier documentation structure: \texttt{PROCESS.md} provides a human-readable executive summary of each development session, while GitHub issues serve as the authoritative, machine-queryable record of all human-agent interactions.

As of this writing, the repository contains 31 GitHub issues spanning 10 development sessions, with each issue labeled according to the scheme described in Section~\ref{sec:methodology-git}. The git history contains 28 semantically versioned commits across 6 release tags (v0.1.0 through v0.6.0), each corresponding to a distinct compliance-relevant action. The repository also contains a visualization toolkit that generates 10 publication-quality charts from git data across the ecosystem, of which 5 are included as figures in this paper (Figures~\ref{fig:repo-comparison}--\ref{fig:commit-patterns}). Together, these records provide sufficient information for an independent team to reproduce the development process or for an auditor to verify that every artifact has a documented provenance chain.

This dual-track approach---git for configuration management, GitHub issues for interaction traceability---mirrors the separation between configuration management (NIST SP 800-53 CM-3) and audit logging (NIST SP 800-53 AU-3) that government frameworks prescribe. The combination ensures that the process is documented at both the artifact level (what changed) and the decision level (why it changed).

% ============================================================
\section{Future Work}
\label{sec:future}

Several directions merit further investigation:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Automated compliance testing}: Integrating AI agents with continuous integration pipelines to validate compliance assertions against code changes.
    \item \textbf{Standard-specific agents}: Training or fine-tuning agents on specific government standards (e.g., a NIST SP 800-171 specialist agent) to improve requirement extraction accuracy.
    \item \textbf{Cross-reference validation}: Building tools that automatically verify citations between compliance artifacts (requirements $\leftrightarrow$ verification $\leftrightarrow$ code).
    \item \textbf{FedRAMP and CMMC application}: Extending the methodology to broader compliance frameworks such as FedRAMP authorization packages and CMMC assessments.
    \item \textbf{Comparative studies}: Quantitative comparison of AI-assisted vs. manual compliance documentation effort across multiple projects and team sizes.
\end{enumerate}

% ============================================================
\section{Conclusion}
\label{sec:conclusion}

This paper has demonstrated a five-phase methodology for combining git version control and AI coding agents to address the challenge of building software that meets government compliance requirements. Through three case studies, we showed that AI agents can produce structurally sound compliance artifacts including requirements specifications, decision memoranda, verification documents, and automated compliance attestations, while the interactive approval model provides the human oversight that government frameworks require.

The key insight is not that AI replaces compliance expertise, but that it \textit{restructures} the compliance workflow. The engineer's role shifts from author to reviewer, the documentation burden decreases without sacrificing rigor, and the multi-agent architecture enables scalable compliance workflows for projects of varying complexity. Critically, the version control and interaction traceability layer---git for configuration management, GitHub issues for human-agent interaction logging---provides the audit evidence that government frameworks demand, mapping directly to NIST SP 800-53 controls CM-3 and AU-3.

This paper itself demonstrates the methodology: it was produced across multiple AI agent sessions, with every human directive and agent action logged as GitHub issues, every change captured in semantically versioned git commits, and the entire process reproducible from the public repository. The fact that an AI agent can assist in producing both the compliance artifacts \textit{and} the auditable process documentation for those artifacts suggests a path toward significantly reducing the overhead of government compliance work.

This work is not a proof of concept. The methodology, agent configurations, and process artifacts presented here are in active use across 16 repositories spanning government compliance, systems engineering, security tooling, and CAD---real projects with real deliverables. The approach produces measurable outcomes: 642~commits, 34,000+ lines of code, 136~release tags, and over 130~compliance artifacts across seven repositories in 26~calendar days---produced by a single engineer with AI agent support. This demonstrates more consistent documentation (fewer gaps, stronger traceability), faster delivery (the review-centric workflow eliminates the authoring bottleneck), and reduced personnel requirements (one engineer with AI agents sustains the documentation overhead that traditionally demands a dedicated compliance team). The engineering experience itself improves when the tedious parts of compliance work are handled by agents and the human focuses on judgment, direction, and review. As government agencies and contractors face increasing pressure to demonstrate compliance across expanding regulatory frameworks, the methodology presented here offers a practical, field-tested foundation for getting real work done.

% ============================================================
\section*{Acknowledgments}

This paper and its supporting artifacts were developed using the methodology it describes, with Claude Code (Anthropic, model: Claude Opus) as the AI agent implementation. All source materials, including the LaTeX source, agent configurations, git history, and process documentation, are available at \url{https://github.com/brucedombrowski/WhitePaper}.

% ============================================================
\bibliographystyle{ieeetr}
\bibliography{references}

\end{document}
