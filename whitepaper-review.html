<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Bruce Dombrowski Independent Researcher GitHub: brucedombrowski" />
  <title>Git and AI Coding Agents for Government Compliance</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    div.abstract {
      margin: 2em 2em 2em 2em;
      text-align: left;
      font-size: 85%;
    }
    div.abstract-title {
      font-weight: bold;
      text-align: center;
      padding: 0;
      margin-bottom: 0.5em;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Git and AI Coding Agents for Government
Compliance</h1>
<p class="author">Bruce Dombrowski<br />
<em>Independent Researcher</em><br />
GitHub: brucedombrowski</p>
<div class="abstract">
<div class="abstract-title">Abstract</div>
<p>Government software development demands rigorous compliance with
federal standards including NIST Special Publications, FIPS
cryptographic requirements, and CUI handling regulations under
32 CFR Part 2002. These requirements impose significant documentation
overhead—formal requirements traceability, decision memoranda,
verification matrices, and regulatory cross-references—that
traditionally consumes substantial engineering effort. This paper
presents a methodology for combining <em>git version control</em> and
<em>AI coding agents</em> to address this burden. Git provides the
tamper-evident audit trail, branching workflow, and configuration
management that government frameworks require; AI coding agents draft
compliance artifacts, generate structured requirements, and produce
traceability documentation under human review. Together, they shift the
engineer’s role from author to reviewer while maintaining the
human-in-the-loop oversight that compliance frameworks demand. We
present a five-phase methodology—from requirements capture through
version-controlled interaction traceability—and evaluate it through
three case studies: a CUI email encryption tool, a formal decision
documentation system, and a Security Verification Toolkit implementing
automated NIST SP 800-53 control verification. Using Claude Code
(Anthropic) as the AI agent implementation, a single engineer produced
642 commits, 34,000+ lines of code, and 136 release tags across seven
repositories in 26 calendar days—including 7 decision memoranda,
29 formally traced requirements, and automated verification of 14 NIST
SP 800-53 controls. The methodology is agent-platform agnostic; the
principles apply to any agentic AI tool with file system access and
human approval workflows.</p>
</div>
</header>
<p><strong>Keywords:</strong> AI coding agents, git, version control,
government compliance, NIST, FIPS, CUI, controlled unclassified
information, large language models, software engineering, federal
information security, configuration management</p>
<h1 id="sec:introduction">Introduction</h1>
<p>Regulated software development imposes a dual burden on developers:
the software must correctly implement domain-specific standards, and the
<em>process</em> of building that software must be formally documented.
This burden is not unique to any single domain—it applies equally to
safety-critical systems (DO-178C, IEC 61508), information-critical
systems (HIPAA, SOX), and the federal information security context
examined in this paper. A tool that encrypts files per FIPS 197 <span
class="citation" data-cites="fips197">(National Institute of Standards
and Technology 2001b)</span> is insufficient if the development team
cannot produce a requirements traceability matrix linking each
implementation decision to the governing standard. This documentation
overhead—decision memoranda, verification documents, requirements
specifications—is where many small teams and independent developers
struggle to meet government expectations.</p>
<p>Two technologies converge to address this gap. First, <em>git version
control</em> provides the tamper-evident, cryptographically hashed
change history that government configuration management standards (NIST
SP 800-53 CM-3) require. Every change is attributed, timestamped, and
linked to its parent state; the commit log serves as a permanent audit
record that cannot be silently altered. Second, <em>AI coding
agents</em>—large language models that operate directly within the
developer’s file system and terminal, reading source files, generating
artifacts, and executing commands under human approval—can draft
compliance documents, suggest standard references, and produce
structured artifacts at a pace that manual authoring cannot match.
However, government work demands accuracy: an incorrect citation to a
NIST Special Publication or a mischaracterized FIPS requirement could
undermine an entire compliance package.</p>
<p>The combination of git and AI coding agents creates a workflow where
the AI drafts and the human reviews, with every interaction captured in
a version-controlled, auditable record. This paper demonstrates this
approach using Claude Code (Anthropic) as the AI agent implementation,
though the methodology applies to any agentic AI tool with file system
access and a human-in-the-loop approval model.</p>
<p>This paper makes the following contributions:</p>
<ol>
<li><p>A five-phase methodology for using AI agents in government
compliance software development, from requirements capture through
version-controlled interaction traceability.</p></li>
<li><p>Three case studies demonstrating AI-assisted development of
compliance artifacts: SendCUIEmail (a CUI encryption tool), a
LaTeX-based decision memoranda system, and a Security Verification
Toolkit implementing automated NIST SP 800-53 control
verification.</p></li>
<li><p>An audit traceability framework using git (configuration
management) and GitHub issues (interaction logging) to provide
bidirectional provenance between human directives and AI-generated
artifacts.</p></li>
<li><p>A standards-based review process mapped to IEEE 1028, NIST SP
800-53, and ISO/IEC 25010, with enforced separation of duties between
authoring and auditing agents.</p></li>
<li><p>A multi-agent architecture for compliance projects, with
role-based separation of duties and quantitative output
analysis.</p></li>
</ol>
<h1 id="sec:background">Background and Related Work</h1>
<h2 id="government-compliance-landscape">Government Compliance
Landscape</h2>
<p>Federal information security is governed by a layered framework of
executive orders, regulations, and technical standards. Executive
Order 13556 established the Controlled Unclassified Information (CUI)
program, implemented through 32 CFR Part 2002 <span class="citation"
data-cites="32cfr2002">(Information Security Oversight Office
2016)</span>. The National Institute of Standards and Technology (NIST)
provides the technical backbone through publications including:</p>
<ul>
<li><p><strong>NIST SP 800-171</strong> <span class="citation"
data-cites="nist800171">(Ross et al. 2020)</span>: Protecting CUI in
Nonfederal Information Systems</p></li>
<li><p><strong>NIST SP 800-53</strong> <span class="citation"
data-cites="nist80053">(Force 2020)</span>: Security and Privacy
Controls for Information Systems</p></li>
<li><p><strong>NIST SP 800-132</strong> <span class="citation"
data-cites="nist800132">(Turan et al. 2010)</span>: Recommendation for
Password-Based Key Derivation</p></li>
<li><p><strong>FIPS 197</strong> <span class="citation"
data-cites="fips197">(National Institute of Standards and Technology
2001b)</span>: Advanced Encryption Standard (AES)</p></li>
<li><p><strong>FIPS 140-2</strong> <span class="citation"
data-cites="fips1402">(National Institute of Standards and Technology
2001a)</span>: Security Requirements for Cryptographic Modules</p></li>
</ul>
<p>Compliance requires not only that software implementations adhere to
these standards, but that organizations maintain documentation
demonstrating adherence—what auditors term “evidence of compliance.”
This evidence typically includes requirements specifications, design
decisions, test plans, and verification matrices that trace each
requirement to its implementation and test.</p>
<h2 id="ai-assisted-software-development">AI-Assisted Software
Development</h2>
<p>The application of large language models to software engineering has
been studied extensively <span class="citation"
data-cites="fan2023llmse">(Fan et al. 2023)</span>. Code generation
tools such as GitHub Copilot, Amazon CodeWhisperer, and Anthropic’s
Claude have demonstrated capability in producing syntactically correct
code across multiple languages. However, the application of LLMs to
<em>compliance-oriented</em> development—where correctness encompasses
not just functional behavior but regulatory adherence—remains
underexplored.</p>
<p>Prior work on AI-assisted documentation generation has focused
primarily on API documentation <span class="citation"
data-cites="khan2022apidoc">(Khan and Uddin 2022)</span> and code
comments. The generation of <em>regulatory</em> documentation—where the
AI must reason about the relationship between code implementations and
published standards—presents distinct challenges including citation
accuracy, regulatory interpretation, and the need for conservative
(rather than creative) text generation.</p>
<p>The current generation of AI coding tools spans a spectrum of
integration depth. <em>Inline completion</em> tools (GitHub Copilot,
Amazon CodeWhisperer, Tabnine) operate within the editor, suggesting
code as the developer types. These tools excel at reducing
keystroke-level effort but lack the broader project context needed for
compliance work—they cannot read a NIST standard reference and produce a
corresponding requirements document. <em>Chat-based</em> tools (ChatGPT,
Gemini) provide conversational interfaces but operate in isolation from
the developer’s file system, requiring manual copy-paste of code and
artifacts. <em>Agentic</em> tools (Claude Code, Cursor, Windsurf, Aider)
operate directly within the developer’s environment, reading and writing
files, executing commands, and maintaining session context. This agentic
architecture is essential for compliance work, where the AI must
simultaneously reason about source code, published standards, and the
traceability relationships between them.</p>
<p>The critical property for compliance applications is an <em>explicit
approval model</em>: every file write, command execution, and code edit
requires human confirmation. While this introduces friction compared to
fully autonomous agents, it produces a natural audit trail of
human-approved actions—precisely the evidence of human oversight that
government compliance frameworks (NIST SP 800-53 AC-5, SA-11) require.
Tool permission systems also enable the separation-of-duties pattern
described in Section <a href="#sec:agents" data-reference-type="ref"
data-reference="sec:agents">7</a>, where review agents are denied write
access at the tool level rather than by convention. Claude Code
implements this model; other agentic tools vary in the granularity of
their approval workflows.</p>
<h2 id="agentic-ai-tool-architecture">Agentic AI Tool Architecture</h2>
<p>AI coding agents operate as command-line tools with access to the
developer’s local environment. The architectural properties essential
for compliance work include:</p>
<ol>
<li><p><strong>File system access</strong>: The agent reads and writes
files directly, enabling it to analyze source code and produce artifacts
in-place.</p></li>
<li><p><strong>Tool use with approval</strong>: Each action (file read,
edit, command execution) requires developer approval, providing the
human oversight that compliance frameworks demand.</p></li>
<li><p><strong>Context persistence</strong>: The agent maintains
conversation context across a session, allowing iterative refinement of
compliance artifacts.</p></li>
<li><p><strong>Instruction files</strong>: Projects include persistent
instruction files (e.g., <code>CLAUDE.md</code> in Claude Code <span
class="citation" data-cites="claudecode">(Anthropic 2025)</span>,
<code>.cursorrules</code> in Cursor) that encode project-specific
compliance requirements across sessions.</p></li>
<li><p><strong>Multi-agent orchestration</strong>: Configurations that
enable multiple specialized agents—each with defined roles, model
selection, and permitted tools—to collaborate on a single
project.</p></li>
<li><p><strong>Session continuity</strong>: Sessions persist across
interruptions, preserving the accumulated compliance context that would
otherwise need to be reconstructed.</p></li>
</ol>
<p>To implement this architecture, the project records its agent
invocation configuration in a version-controlled instruction file.
Table <a href="#tab:cli-concepts" data-reference-type="ref"
data-reference="tab:cli-concepts">1</a> maps the methodology-level
concepts to their implementation in the tool used in this paper’s case
studies.</p>
<div id="tab:cli-concepts">
<table>
<caption>Methodology concepts and their implementation (Claude
Code)</caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Concept</strong></th>
<th style="text-align: left;"><strong>Implementation</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Multi-agent config</td>
<td style="text-align: left;"><code>--agents</code> (JSON file)</td>
</tr>
<tr>
<td style="text-align: left;">Model selection</td>
<td style="text-align: left;"><code>--model</code> (reasoning vs.
throughput)</td>
</tr>
<tr>
<td style="text-align: left;">Tool restrictions</td>
<td style="text-align: left;"><code>--allowedTools</code> (per
role)</td>
</tr>
<tr>
<td style="text-align: left;">Session resume</td>
<td style="text-align: left;"><code>--continue</code></td>
</tr>
<tr>
<td style="text-align: left;">Audit logging</td>
<td style="text-align: left;"><code>--verbose</code></td>
</tr>
</tbody>
</table>
</div>
<h1 id="sec:methodology">Methodology</h1>
<p>We developed a methodology for AI-assisted government compliance
development organized around five phases, illustrated in Figure <a
href="#fig:methodology" data-reference-type="ref"
data-reference="fig:methodology">1</a>. Each phase leverages AI coding
agent capabilities while maintaining the human-in-the-loop oversight
essential to compliance work.</p>
<figure id="fig:methodology" data-latex-placement="htbp">

<figcaption>Five-phase methodology for AI-assisted compliance
development. Human review occurs at every phase transition.</figcaption>
</figure>
<h2 id="phase-1-requirements-capture">Phase 1: Requirements Capture</h2>
<p>Government projects begin with requirements derived from applicable
standards. In our methodology, the developer identifies the governing
standards (e.g., NIST SP 800-132 for key derivation) and instructs the
AI agent to generate a structured requirements document.</p>
<p>The agent produces requirements in machine-readable JSON format,
enabling downstream tooling to generate formatted documents and
traceability matrices. Each requirement includes:</p>
<ul>
<li><p>A unique identifier (e.g., <code>REQ-1.1</code>)</p></li>
<li><p>The governing standard and section reference</p></li>
<li><p>The requirement text</p></li>
<li><p>Classification as mandatory or recommended</p></li>
<li><p>Verification method (inspection, test, analysis)</p></li>
</ul>
<p>Listing <a href="#lst:req-json" data-reference-type="ref"
data-reference="lst:req-json">[lst:req-json]</a> shows an excerpt from
the SendCUIEmail requirements document, generated with AI agent
assistance and reviewed by the developer.</p>
<pre id="lst:req-json" style="json"
data-caption="Requirements specification excerpt (REQ-2026-001)"
data-label="lst:req-json"><code>{
  &quot;id&quot;: &quot;REQ-1.1&quot;,
  &quot;standard&quot;: &quot;FIPS 197&quot;,
  &quot;section&quot;: &quot;Section 1&quot;,
  &quot;text&quot;: &quot;The tool SHALL use the Advanced
    Encryption Standard (AES) algorithm
    for all file encryption operations.&quot;,
  &quot;priority&quot;: &quot;mandatory&quot;,
  &quot;verification&quot;: &quot;inspection&quot;
}</code></pre>
<p>Requirement text uses RFC 2119 <span class="citation"
data-cites="rfc2119">(Bradner 1997)</span> keywords (SHALL, SHOULD, MAY)
to distinguish mandatory from recommended requirements, following the
convention established in IETF and NIST publications. The developer’s
role shifts from <em>authoring</em> requirements to <em>reviewing</em>
them—verifying that the AI’s interpretation of the standard is correct
and that no requirements are omitted. This review-centric workflow is
faster than drafting from scratch while preserving the technical
judgment that compliance demands.</p>
<h2 id="phase-2-implementation-with-compliance-awareness">Phase 2:
Implementation with Compliance Awareness</h2>
<p>During implementation, the AI agent operates within the project’s
instruction files, which encode compliance standards and architectural
constraints. <code>CLAUDE.md</code> provides project-wide instructions
(build commands, repository scope, conventions), while
<code>AGENTS.md</code> defines role-specific compliance context
(applicable standards, verification methods, regulatory constraints).
Both files persist across sessions, ensuring that every agent invocation
begins with the correct compliance posture. Listing <a
href="#lst:agentsmd" data-reference-type="ref"
data-reference="lst:agentsmd">[lst:agentsmd]</a> shows the compliance
context from the SendCUIEmail project:</p>
<pre id="lst:agentsmd" style="powershell"
data-caption="AGENTS.md compliance context excerpt"
data-label="lst:agentsmd"><code>## Compliance Standards

- **FIPS 140-2**: AES-256-CBC encryption
- **NIST SP 800-132**: PBKDF2-HMAC-SHA256
    key derivation (100,000 iterations)
- **NIST SP 800-171**: CUI handling
- **32 CFR Part 2002**: CUI marking</code></pre>
<p>This ensures that every agent session begins with awareness of the
applicable standards, reducing the risk of non-compliant
suggestions.</p>
<h2 id="phase-3-decision-documentation">Phase 3: Decision
Documentation</h2>
<p>Government compliance frequently requires documenting <em>why</em> a
particular approach was chosen, not merely <em>what</em> was
implemented. Decision memoranda serve this purpose. In our methodology,
when the developer makes a design choice (e.g., selecting Cinzel over
Trajan Bold for CUI headers, or choosing TikZ over PDF manipulation for
form layout), they instruct the agent to generate a formal decision
memo.</p>
<p>The LaTeX/Decisions repository implements a template-wrapper pattern
where each decision memo defines metadata variables and content, then
includes a shared template. Listing <a href="#lst:dm-pattern"
data-reference-type="ref"
data-reference="lst:dm-pattern">[lst:dm-pattern]</a> illustrates this
separation:</p>
<pre id="lst:dm-pattern" style="powershell"
data-caption="Decision memo template pattern"
data-label="lst:dm-pattern"><code>\newcommand{\UniqueID}{DM-2026-002}
\newcommand{\DocumentDate}{January 19, 2026}
\newcommand{\AuthorName}{PDF Tools Working Group}
\newcommand{\SubjectField}{Font Selection for
    CUI Header Text}
\newcommand{\dmContent}{...}
\input{_template.tex}</code></pre>
<p>This pattern enables the AI agent to produce new decision memos by
following the established template, ensuring consistency across the
documentation package.</p>
<h2 id="phase-4-verification">Phase 4: Verification</h2>
<p>The verification phase produces documents that map each requirement
to its implementation evidence. The agent reads the source code, locates
the relevant implementation for each requirement, and generates a
verification matrix with file paths, line numbers, and explanatory
text.</p>
<p>Table <a href="#tab:verification-excerpt" data-reference-type="ref"
data-reference="tab:verification-excerpt">2</a> shows an excerpt from
the SendCUIEmail verification document.</p>
<div id="tab:verification-excerpt">
<table>
<caption>Verification matrix excerpt (VER-2026-001)</caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Req.</strong></th>
<th style="text-align: left;"><strong>Evidence</strong></th>
<th style="text-align: left;"><strong>Method</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">REQ-1.1</td>
<td style="text-align: left;"><code>Encrypt.ps1</code>:
<code>[Aes]::Create()</code> call</td>
<td style="text-align: left;">Inspection</td>
</tr>
<tr>
<td style="text-align: left;">REQ-1.2</td>
<td style="text-align: left;"><code>$KEY_SIZE = 32</code> (256
bits)</td>
<td style="text-align: left;">Inspection</td>
</tr>
<tr>
<td style="text-align: left;">REQ-2.3</td>
<td style="text-align: left;"><code>$ITERATIONS = 100000</code></td>
<td style="text-align: left;">Inspection</td>
</tr>
<tr>
<td style="text-align: left;">REQ-3.1</td>
<td
style="text-align: left;"><span><code>RandomNumberGenerator</code></span>
usage</td>
<td style="text-align: left;">Inspection</td>
</tr>
</tbody>
</table>
</div>
<h2 id="sec:methodology-git">Phase 5: Version Control and Interaction
Traceability</h2>
<p>The preceding development phases produce artifacts, but compliance
also demands <em>evidence of process</em>—a verifiable record of who
made which decisions, when changes were introduced, and how human-agent
interactions shaped the final deliverables. We use git version control
and GitHub issues as complementary traceability mechanisms.</p>
<h3 id="git-as-audit-trail">Git as Audit Trail</h3>
<p>Every meaningful action—creating a requirements document, fixing a
review finding, adding a compliance scan—is captured as an atomic git
commit on the project’s main branch. Each commit message describes the
compliance-relevant change (e.g., “Fix all 13 review findings from
issue #1; add QA standards framework”). This produces a linear,
tamper-evident history that auditors can inspect with standard tooling
(<code>git log</code>, <code>git diff</code>).</p>
<p>Git’s properties align directly with government configuration
management requirements. NIST SP 800-53 CM-3 (Configuration Change
Control) requires organizations to “document, approve, and track changes
to the system” <span class="citation" data-cites="nist80053">(Force
2020)</span>. The git commit log serves as this change record: each
commit is cryptographically hashed, timestamped, attributed to an
author, and linked to its parent commits. Unlike informal change logs,
git history cannot be silently altered without breaking the hash
chain.</p>
<p>The project uses Semantic Versioning (SemVer) with a
<code>CHANGELOG.md</code> following the Keep a Changelog convention.
Version numbers encode the significance of changes: major versions for
structural reorganization, minor versions for new content, and patch
versions for corrections. Each release is tagged
(<code>git tag -a vX.Y.Z</code>) and the changelog entries reference the
GitHub issues that motivated each change. This provides a human-readable
change history that complements the machine-level detail in the git
log.</p>
<p>The Security Verification Toolkit case study (Section <a
href="#sec:casestudy-toolkit" data-reference-type="ref"
data-reference="sec:casestudy-toolkit">6</a>) embeds the git commit hash
directly into its compliance attestation PDFs, binding each attestation
to a specific, reproducible configuration state.</p>
<h3 id="github-issues-as-interaction-log">GitHub Issues as Interaction
Log</h3>
<p>While git captures <em>what changed</em>, GitHub issues capture
<em>why it changed</em> and <em>who directed the change</em>. All
human-agent interactions in this project are logged as GitHub issues
using a structured labeling scheme:</p>
<ul>
<li><p><code>human-prompt</code>: A human directive to the AI agent
(e.g., “expand agents.json with additional agent roles”)</p></li>
<li><p><code>agent-output</code>: Agent-generated analysis or findings
(e.g., “13 review findings per IEEE 1028 inspection”)</p></li>
<li><p><code>decision</code>: A design or process decision with
rationale (e.g., “IT security standards are standard review
criteria”)</p></li>
</ul>
<p>This labeling scheme creates a queryable audit record. An auditor can
filter by <code>human-prompt</code> to see every directive the human
issued, by <code>agent-output</code> to see every AI-generated analysis,
or by <code>decision</code> to trace the rationale for each design
choice. The combination provides bidirectional traceability between
human intent and AI action—a key requirement when demonstrating
human-in-the-loop oversight to government auditors.</p>
<p>All five agents in the multi-agent configuration (Section <a
href="#sec:agents" data-reference-type="ref"
data-reference="sec:agents">7</a>) include interaction logging in their
system prompts, requiring them to create GitHub issues for every
substantive human-agent exchange. This ensures that the audit trail is
comprehensive regardless of which agent is active.</p>
<h1 id="sec:casestudy-sendcui">Case Study: SendCUIEmail</h1>
<h2 id="project-overview">Project Overview</h2>
<p>SendCUIEmail <span class="citation"
data-cites="sendcuiemail">(Dombrowski 2026b)</span> is a
PowerShell-based tool for encrypting files before email transmission,
under active development (currently v0.17.3, pre-release). It is
designed for environments where Public Key Infrastructure (PKI) or
S/MIME certificate exchange is impractical. The tool addresses a common
gap in federal and contractor environments: the need to transmit CUI
securely when the only available channel is unencrypted email.</p>
<p>The project’s compliance scope spans six federal standards and
regulations:</p>
<ol>
<li><p><strong>FIPS 197</strong> <span class="citation"
data-cites="fips197">(National Institute of Standards and Technology
2001b)</span>: AES algorithm specification</p></li>
<li><p><strong>FIPS 140-2</strong> <span class="citation"
data-cites="fips1402">(National Institute of Standards and Technology
2001a)</span>: Cryptographic module validation</p></li>
<li><p><strong>NIST SP 800-132</strong> <span class="citation"
data-cites="nist800132">(Turan et al. 2010)</span>: Password-based key
derivation</p></li>
<li><p><strong>NIST SP 800-38A</strong> <span class="citation"
data-cites="nist80038a">(Dworkin 2001)</span>: Block cipher modes of
operation</p></li>
<li><p><strong>NIST SP 800-90A</strong> <span class="citation"
data-cites="nist80090a">(Barker and Kelsey 2015)</span>: Random number
generation</p></li>
<li><p><strong>32 CFR Part 2002</strong> <span class="citation"
data-cites="32cfr2002">(Information Security Oversight Office
2016)</span>: CUI marking and handling</p></li>
</ol>
<h2 id="ai-assisted-artifacts">AI-Assisted Artifacts</h2>
<p>Over the course of development, the AI agent assisted in producing
the following compliance artifacts:</p>
<h3 id="requirements-document-req-2026-001">Requirements Document
(REQ-2026-001)</h3>
<p>A JSON-formatted requirements specification containing 29
requirements across six categories: encryption algorithm, key
derivation, random number generation, password handling, file format,
and platform requirements. The JSON source-of-truth enables automated
generation of formatted PDF documents via a Python build script.</p>
<h3 id="decision-memoranda-dm-2026-001-through-dm-2026-007">Decision
Memoranda (DM-2026-001 through DM-2026-007)</h3>
<p>Seven formal decision memos documenting design choices:</p>
<ul>
<li><p><strong>DM-001</strong>: Cross-platform support strategy</p></li>
<li><p><strong>DM-002</strong>: File size limit rationale
(10 MB)</p></li>
<li><p><strong>DM-003</strong>: Password transmission method
(out-of-band per NIST SP 800-63B <span class="citation"
data-cites="nist80063b">(Grassi et al. 2017)</span>)</p></li>
<li><p><strong>DM-004</strong>: Verification document numbering
scheme</p></li>
<li><p><strong>DM-005</strong>: Multi-category CUI support per
32 CFR 2002.20(a)(3)</p></li>
<li><p><strong>DM-006</strong>: Beta readiness assessment</p></li>
<li><p><strong>DM-007</strong>: Recipient instruction format selection
(HTML)</p></li>
</ul>
<h3 id="verification-document-ver-2026-001">Verification Document
(VER-2026-001)</h3>
<p>A line-by-line code verification mapping all 29 requirements to
specific implementation evidence in the source code, including file
paths, function names, and configuration values.</p>
<h2 id="cryptographic-implementation">Cryptographic Implementation</h2>
<p>The core encryption implementation demonstrates how AI-assisted
development can produce compliant code. The encrypted file format
is:</p>
<p><span class="math display">\[\begin{multline}
\text{Output} = \text{Salt}_{128} \| \text{IV}_{128} \| {} \\
\text{AES-256-CBC}(K, \text{IV}, \text{Plaintext})
\end{multline}\]</span></p>
<p>where <span class="math inline">\(K\)</span> is derived via
PBKDF2-HMAC-SHA256:</p>
<p><span class="math display">\[\begin{equation}
K = \text{PBKDF2}(\text{password}, \text{Salt}, 100000, 256)
\end{equation}\]</span></p>
<p>The implementation uses exclusively platform-provided cryptographic
libraries (<code>System.Security.Cryptography</code>), avoiding
third-party dependencies that would complicate FIPS validation. When
Windows FIPS mode is enabled, the tool leverages CMVP-validated
cryptographic modules (e.g., Certificate #4515, Kernel Mode
Cryptographic Primitives Library, validated under FIPS 140-2 on Windows
10; specific certificate numbers vary by Windows version).</p>
<h2 id="recipient-experience-design">Recipient Experience Design</h2>
<p>A significant AI-assisted design contribution was the recipient
decryption workflow. The tool generates a self-contained HTML
instruction document (<code>Decrypt_Instructions.html</code>) with an
embedded PowerShell one-liner, shown in simplified form in Listing <a
href="#lst:oneliner" data-reference-type="ref"
data-reference="lst:oneliner">[lst:oneliner]</a>:</p>
<pre id="lst:oneliner" style="powershell"
data-caption="Decryption logic (simplified from production code)"
data-label="lst:oneliner"><code>$f=Read-Host &quot;File&quot;
$p=Read-Host &quot;Password&quot;
$d=[IO.File]::ReadAllBytes($f)
$k=[Rfc2898DeriveBytes]::new(
    $p,$d[0..15],100000,&quot;SHA256&quot;)
$a=[Aes]::Create()
$a.Key=$k.GetBytes(32)
$a.IV=$d[16..31]
$c=$a.CreateDecryptor()
    .TransformFinalBlock($d,32,$d.Length-32)
[IO.File]::WriteAllBytes(
    ($f-replace&#39;\.Locked$&#39;,&#39;&#39;),$c)</code></pre>
<p>This design requires no software installation by recipients—only
PowerShell, which is built into every modern Windows installation. The
production code uses <code>SecureString</code> with
<code>SecureStringToBSTR</code> conversion and file picker dialogs; the
listing above is a functionally correct simplification using plaintext
password input for clarity. The AI agent helped iterate on the
production one-liner to minimize its length while maintaining compliance
with the cryptographic parameter requirements.</p>
<h1 id="sec:casestudy-decisions">Case Study: Decision Documentation
System</h1>
<p>The LaTeX/Decisions repository (v0.4, under active development)
demonstrates AI-assisted creation of a reusable documentation system for
formal decision memoranda. Government programs frequently require
Decision Memoranda (DMs) to document technical and policy choices with
traceable rationale.</p>
<h2 id="template-architecture">Template Architecture</h2>
<p>The system uses a template-wrapper pattern where a shared base
template (<code>_template.tex</code>) defines the document
layout—headers with organizational logo, footers with document ID and
page numbering, and standardized section formatting—while individual
decision documents supply metadata and content through LaTeX command
definitions.</p>
<p>This separation of concerns enables AI agents to produce new decision
memos by populating the established template structure, ensuring visual
and structural consistency without requiring the agent to understand the
full LaTeX layout implementation.</p>
<h2 id="sf901-cui-coversheet-compliance">SF901 CUI Coversheet
Compliance</h2>
<p>Three decision memos (DM-2026-001 through DM-2026-003) document the
technical approach to generating Standard Form 901 CUI coversheets:</p>
<ol>
<li><p><strong>Implementation approach</strong>: LaTeX template
recreation rather than PDF manipulation, chosen for alignment with
existing infrastructure and independence from external tools.</p></li>
<li><p><strong>Font selection</strong>: Cinzel (open-source, SIL OFL)
chosen over Trajan Bold (commercial) for the CUI header, balancing
visual fidelity with licensing constraints.</p></li>
<li><p><strong>Layout strategy</strong>: TikZ with absolute positioning
for pixel-precise form reproduction, justified by the form’s stability
(unchanged since November 2018 per GSA records).</p></li>
</ol>
<p>Each decision memo follows the format required by many government
programs: identification of options considered, evaluation criteria,
selected approach, and rationale with regulatory references.</p>
<h1 id="sec:casestudy-toolkit">Case Study: Security Verification
Toolkit</h1>
<p>The third case study examines the Security Verification Toolkit <span
class="citation" data-cites="securitytoolkit">(Dombrowski 2026a)</span>,
a pure-Bash security scanning and compliance documentation system under
active development (currently v2.7.3) that automates the verification of
federal security controls. Like all three case studies in this paper,
the toolkit is under active development and is presented as a case study
in AI-assisted compliance tooling, not as a finished product. Unlike
SendCUIEmail (which implements a single compliance function) or the
Decision Documentation System (which manages process artifacts), the
toolkit addresses the <em>continuous compliance verification</em>
challenge: demonstrating ongoing adherence to NIST SP 800-53 and NIST SP
800-171 controls through automated scanning and attestation
generation.</p>
<h2 id="scope-and-standards">Scope and Standards</h2>
<p>The toolkit implements 14 NIST SP 800-53 controls and 11 NIST SP
800-171 controls across eight security control families, with each scan
mapped to its governing control in machine-readable JSON. The standards
addressed include:</p>
<ul>
<li><p><strong>NIST SP 800-53</strong> <span class="citation"
data-cites="nist80053">(Force 2020)</span>: AU-2/3 (Audit Events and
Records), CA-2 (Assessment), CM-6/8 (Configuration), MP-6 (Media
Sanitization), RA-5 (Vulnerability Scanning), SA-11 (Developer Testing),
SC-8 (Transmission Protection), SI-2/3/4/5/12 (Information
Integrity)</p></li>
<li><p><strong>NIST SP 800-171</strong> <span class="citation"
data-cites="nist800171">(Ross et al. 2020)</span>: 11 corresponding CUI
protection requirements</p></li>
<li><p><strong>NIST SP 800-88</strong> <span class="citation"
data-cites="nist80088">(Kissel et al. 2014)</span>: Media sanitization
(secure deletion)</p></li>
<li><p><strong>BOD 22-01</strong> <span class="citation"
data-cites="bod2201">(Cybersecurity and Infrastructure Security Agency
2021)</span>: CISA Known Exploited Vulnerabilities
cross-referencing</p></li>
<li><p><strong>FIPS 199</strong> <span class="citation"
data-cites="fips199">(National Institute of Standards and Technology
2004)</span>: Security categorization of federal information</p></li>
</ul>
<h2 id="requirements-traceability">Requirements Traceability</h2>
<p>The toolkit maintains a complete traceability chain in JSON
format:</p>
<p><span class="math display">\[\begin{equation*}
\text{Requirement} \rightarrow \text{NIST Control} \rightarrow
\text{Script} \rightarrow \text{Test} \rightarrow \text{Evidence}
\end{equation*}\]</span></p>
<p>A <code>mapping.json</code> file links 14 functional requirements
(FR-001 through FR-014) to NIST controls, implementation scripts, and
test cases, navigable in three directions: by script, by NIST 800-53
control, and by NIST 800-171 control. This bidirectional traceability
enables auditors to verify compliance from any starting point—a
requirement that many government programs mandate but few tools
automate.</p>
<p>The AI agent assisted in generating this traceability framework,
producing the initial JSON mappings from the NIST control catalog and
iterating with the developer to ensure completeness. The
machine-readable format enables downstream automation: generating
formatted traceability reports, validating that no requirements are
orphaned, and detecting when code changes break previously-verified
controls.</p>
<h2 id="automated-attestation-generation">Automated Attestation
Generation</h2>
<p>A distinguishing feature of the toolkit is its automated generation
of formal compliance attestations as PDFs via LaTeX templates. After
each scan run, the system produces timestamped, checksummed attestation
documents suitable for inclusion in government compliance packages. Each
attestation includes:</p>
<ul>
<li><p>Toolkit version and git commit hash (configuration
management)</p></li>
<li><p>Scan timestamp in ISO 8601 UTC</p></li>
<li><p>SHA-256 checksums of all scan outputs</p></li>
<li><p>NIST control mappings for each finding</p></li>
<li><p>CUI markings where applicable</p></li>
</ul>
<p>The toolkit’s design philosophy—<em>“You are only as good as your
last scan”</em>—enforces that every scan run overwrites previous
results, preventing stale attestations from being presented as current
evidence. This maps directly to the continuous monitoring requirements
of NIST CA-7.</p>
<h2 id="codebase-structure-evolution">Codebase Structure Evolution</h2>
<p>Code survival analysis using git-of-theseus reveals how the toolkit’s
codebase structure evolved over its 463-commit history. Figure <a
href="#fig:theseus-dirs" data-reference-type="ref"
data-reference="fig:theseus-dirs">2</a> shows that the
<code>scripts/</code> (implementation) and <code>tests/</code>
(verification) directories grew in lockstep throughout the development
period—from approximately 10,000 and 3,000 lines respectively at the
start of the analyzed period to 27,000 and 8,000 lines at the time of
writing. This parallel growth indicates disciplined test coverage
practices maintained throughout AI-assisted development: new scanning
capabilities were consistently accompanied by corresponding test
cases.</p>
<figure id="fig:theseus-dirs" data-latex-placement="htbp">
<embed src="visualizations/theseus_directories.pdf" />
<figcaption>Security Verification Toolkit codebase structure evolution
(git-of-theseus). The <code>scripts/</code> and <code>tests/</code>
directories grow in lockstep, indicating consistent test coverage
practices throughout AI-assisted development.</figcaption>
</figure>
<h2 id="multi-agent-development">Multi-Agent Development</h2>
<p>The toolkit itself was developed using a multi-agent architecture
with defined roles: Lead Systems Engineer, QA Engineer, Windows
Developer, Documentation Engineer, and Lead Software Developer. Agent
coordination uses GitHub issues as the communication channel—the same
interaction logging pattern adopted in this paper’s methodology. This
represents a mature implementation of the multi-agent compliance
workflow described in Section <a href="#sec:agents"
data-reference-type="ref" data-reference="sec:agents">7</a>, validated
across 94 version tags and over 140 GitHub issues.</p>
<h1 id="sec:agents">Multi-Agent Workflow</h1>
<p>Multi-agent orchestration enables workflows where multiple
specialized AI agents collaborate on a project, each with defined roles,
permitted tools, and compliance context. For government compliance work,
we propose the role-based agent architecture shown in Figure <a
href="#fig:agents" data-reference-type="ref"
data-reference="fig:agents">3</a>.</p>
<figure id="fig:agents" data-latex-placement="htbp">

<figcaption>Multi-agent architecture for compliance projects. Teal
agents use throughput-optimized models; blue/orange agents use
reasoning-optimized models. The review agent has read-only access (NIST
SP 800-53 AC-5). All agents log interactions to GitHub
issues.</figcaption>
</figure>
<h2 id="agent-roles">Agent Roles</h2>
<ol>
<li><p><strong>Project Setup Agent</strong>: Initializes repository
structure, creates <code>CLAUDE.md</code> with compliance context,
establishes documentation templates and directory layout.</p></li>
<li><p><strong>Requirements Agent</strong>: Analyzes governing standards
and generates structured requirements documents in JSON format.</p></li>
<li><p><strong>Implementation Agent</strong>: Writes compliant code
within the constraints defined by the requirements and
<code>CLAUDE.md</code> context.</p></li>
<li><p><strong>Documentation Agent</strong>: Produces decision
memoranda, verification documents, and traceability matrices.</p></li>
<li><p><strong>Review Agent</strong>: Audits artifacts for completeness,
citation accuracy, and cross-reference integrity.</p></li>
</ol>
<h2 id="agent-configuration">Agent Configuration</h2>
<p>Agent definitions are stored in a JSON configuration file that
specifies each agent’s role, model selection, permitted tools, and a
detailed system prompt encoding compliance context. Table <a
href="#tab:agent-config" data-reference-type="ref"
data-reference="tab:agent-config">[tab:agent-config]</a> summarizes the
five-agent configuration developed for this paper.</p>
<div class="table*">
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Agent</strong></th>
<th style="text-align: left;"><strong>Model</strong></th>
<th style="text-align: left;"><strong>Phase</strong></th>
<th style="text-align: left;"><strong>QA Standard</strong></th>
<th style="text-align: left;"><strong>Key Capability</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">project-setup</td>
<td style="text-align: left;">Throughput</td>
<td style="text-align: left;">Setup</td>
<td style="text-align: left;">—</td>
<td style="text-align: left;">Repo structure, build config,
templates</td>
</tr>
<tr>
<td style="text-align: left;">requirements</td>
<td style="text-align: left;">Reasoning</td>
<td style="text-align: left;">Phase 1</td>
<td style="text-align: left;">IEEE 29148</td>
<td style="text-align: left;">Standard interpretation, JSON
requirements</td>
</tr>
<tr>
<td style="text-align: left;">implementation</td>
<td style="text-align: left;">Throughput</td>
<td style="text-align: left;">Phase 2</td>
<td style="text-align: left;">—</td>
<td style="text-align: left;">Compliant code within REQ constraints</td>
</tr>
<tr>
<td style="text-align: left;">documentation</td>
<td style="text-align: left;">Throughput</td>
<td style="text-align: left;">Phases 3–4</td>
<td style="text-align: left;">MIL-STD-498</td>
<td style="text-align: left;">Decision memos, verification docs,
LaTeX</td>
</tr>
<tr>
<td style="text-align: left;">review</td>
<td style="text-align: left;">Reasoning</td>
<td style="text-align: left;">Cross-cutting</td>
<td style="text-align: left;">IEEE 1028, NIST 800-53 AC-5</td>
<td style="text-align: left;">Audit with no write access
(read-only)</td>
</tr>
</tbody>
</table>
<p><em>Note:</em> Phase 5 (Version Control) is performed by all agents
via GitHub issue logging. The review agent operates across phases rather
than within a single phase.</p>
</div>
<p>Model selection reflects the cognitive demands of each role: the
<code>requirements</code> and <code>review</code> agents use a
reasoning-optimized model for its stronger performance on regulatory
interpretation and cross-reference validation, while
<code>implementation</code> and <code>documentation</code> use a
throughput-optimized model for its favorable speed-to-quality ratio on
structured, template-following tasks. In our implementation, these
correspond to Anthropic’s Opus and Sonnet models respectively, but the
pattern applies to any model family with tiered capability levels.
Notably, the <code>review</code> agent is denied write and edit tools,
enforcing a separation-of-duties principle where auditors identify
problems but do not fix them.</p>
<h2 id="workflow-orchestration">Workflow Orchestration</h2>
<p>The multi-agent workflow proceeds through the five phases described
in Section <a href="#sec:methodology" data-reference-type="ref"
data-reference="sec:methodology">3</a>, with each agent operating within
its defined scope. The key advantage of this architecture is <em>context
isolation</em>: the requirements agent does not need the full
implementation context, and the documentation agent can focus on
artifact generation without the overhead of the full codebase in its
context window.</p>
<p>This isolation is particularly valuable for government projects where
compliance documentation can be extensive—a full NIST SP 800-171
assessment may reference over 100 security requirements, and maintaining
all of these in a single agent context is impractical. The
separation-of-duties between the <code>documentation</code> and
<code>review</code> agents also mirrors the organizational controls
common in government programs, where the author of a compliance artifact
should not be the sole reviewer.</p>
<h2 id="sec:scrum">Scrum-Based Agent Orchestration</h2>
<p>The pipeline model described above—where agents execute sequentially
through defined phases—is effective for linear compliance workflows but
does not accommodate the iterative, feedback-driven nature of real-world
software development. An alternative orchestration model maps AI agents
to a Scrum team structure, drawing on the Scrum Guide <span
class="citation" data-cites="scrumguide">(Schwaber and Sutherland
2020)</span> framework that is widely adopted in both commercial and
government software programs.</p>
<p>In this model, AI agents assume Scrum roles:</p>
<ul>
<li><p><strong>Product Owner agent</strong>: Maintains the product
backlog (GitHub issues), prioritizes work items based on compliance risk
and stakeholder value, and defines acceptance criteria. Uses a
reasoning-optimized model for its judgment over regulatory
priorities.</p></li>
<li><p><strong>Scrum Master agent</strong>: Facilitates sprint
execution, identifies impediments (blocked issues, failing scans,
unresolved review findings), and ensures the team adheres to process
standards. Monitors GitHub issues for stalled work and escalates to the
human.</p></li>
<li><p><strong>Developer agents</strong>: One or more implementation and
documentation agents that pull work from the sprint backlog and produce
deliverables. Use a throughput-optimized model for structured
tasks.</p></li>
</ul>
<p>This Scrum-based approach offers several advantages for compliance
projects:</p>
<ol>
<li><p><strong>Iterative refinement</strong>: Rather than producing all
requirements before any implementation begins, the team works in sprints
where each iteration can incorporate feedback from the previous sprint’s
review—mirroring the review-centric workflow discussed in Section <a
href="#sec:discussion" data-reference-type="ref"
data-reference="sec:discussion">8</a>.</p></li>
<li><p><strong>Backlog as audit trail</strong>: The GitHub issue backlog
serves dual purpose: it is both the Scrum product backlog and the
compliance audit trail (Section <a href="#sec:methodology-git"
data-reference-type="ref" data-reference="sec:methodology-git">3.5</a>).
Each sprint produces a traceable increment of compliance
artifacts.</p></li>
<li><p><strong>Human as stakeholder</strong>: The human engineer acts as
the primary stakeholder (and may also serve as the Product Owner),
providing direction at sprint boundaries rather than approving every
individual action. This scales the human-in-the-loop model to larger
projects.</p></li>
<li><p><strong>Government familiarity</strong>: Scrum and Agile
methodologies are already adopted across federal agencies under guidance
such as the GSA 18F Agile Practices Guide and the DoD Agile Software
Acquisition Guidebook, reducing the organizational friction of adopting
AI-assisted workflows.</p></li>
</ol>
<p>A reference implementation of this Scrum-based agent architecture is
under development at <a href="https://github.com/brucedombrowski/Scrum"
class="uri">https://github.com/brucedombrowski/Scrum</a>, applying the
Scrum Guide’s ceremonies and artifacts to AI agent multi-agent
orchestration. This represents a natural evolution from the sequential
pipeline model to a more flexible, sprint-based orchestration that
better accommodates the iterative nature of compliance development.</p>
<h2 id="ecosystem-architecture">Ecosystem Architecture</h2>
<p>The multi-agent workflow, Scrum orchestration, and underlying process
framework are maintained as a set of interrelated repositories with
clean separation of concerns:</p>
<ul>
<li><p><strong>systems-engineering</strong> (<a
href="https://github.com/brucedombrowski/systems-engineering"
class="uri">https://github.com/brucedombrowski/systems-engineering</a>):
Defines <em>how</em> work is done—the five-phase process, standards
framework, traceability model, and artifact conventions.</p></li>
<li><p><strong>ai-agents</strong> (<a
href="https://github.com/brucedombrowski/ai-agents"
class="uri">https://github.com/brucedombrowski/ai-agents</a>): Defines
<em>who</em> does the work—model-agnostic agent role templates with
vendor-specific implementations.</p></li>
<li><p><strong>Scrum</strong> (<a
href="https://github.com/brucedombrowski/Scrum"
class="uri">https://github.com/brucedombrowski/Scrum</a>): Defines
<em>when</em> work happens—sprint cadence, backlog management, and Scrum
ceremonies.</p></li>
</ul>
<p>Individual project repositories (SendCUIEmail, security-toolkit, this
white paper) consume these shared definitions while maintaining
project-specific instructions. This architecture allows the process,
agent templates, and orchestration model to evolve independently and be
adopted incrementally by new projects.</p>
<h1 id="sec:discussion">Discussion</h1>
<h2 id="quality-of-ai-generated-compliance-artifacts">Quality of
AI-Generated Compliance Artifacts</h2>
<p>Our experience indicates that AI coding agents produce compliance
artifacts that are <em>structurally sound</em> but require careful human
review for <em>substantive accuracy</em>. The AI reliably generates:</p>
<ul>
<li><p>Correct document structure and formatting</p></li>
<li><p>Appropriate standard references (e.g., citing NIST SP 800-132 for
PBKDF2)</p></li>
<li><p>Reasonable requirement decomposition</p></li>
<li><p>Accurate code-to-requirement tracing when given source
access</p></li>
</ul>
<p>Areas requiring human review include:</p>
<ul>
<li><p><em>Regulatory interpretation</em>: Whether a requirement is
“mandatory” vs. “recommended” per the governing standard</p></li>
<li><p><em>Completeness</em>: Whether all applicable requirements from a
standard have been captured</p></li>
<li><p><em>Citation precision</em>: Verifying specific section numbers
within standards</p></li>
<li><p><em>Organizational context</em>: Tailoring requirements to the
specific compliance posture of the organization</p></li>
</ul>
<h2 id="quantitative-output-analysis">Quantitative Output Analysis</h2>
<p>Table <a href="#tab:output-metrics" data-reference-type="ref"
data-reference="tab:output-metrics">3</a> summarizes the measurable
output of the AI-assisted methodology across the three case studies and
supporting infrastructure, produced by a single engineer over
26 calendar days.</p>
<div id="tab:output-metrics">
<table>
<caption>Aggregate output metrics (26-day period, single
engineer)</caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Metric</strong></th>
<th style="text-align: right;"><strong>Value</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Git repositories</td>
<td style="text-align: right;">7</td>
</tr>
<tr>
<td style="text-align: left;">Total commits</td>
<td style="text-align: right;">642</td>
</tr>
<tr>
<td style="text-align: left;">Lines of code</td>
<td style="text-align: right;">34,016</td>
</tr>
<tr>
<td style="text-align: left;">Release tags</td>
<td style="text-align: right;">136</td>
</tr>
<tr>
<td style="text-align: left;">Decision memoranda</td>
<td style="text-align: right;">7</td>
</tr>
<tr>
<td style="text-align: left;">Requirements (JSON)</td>
<td style="text-align: right;">29</td>
</tr>
<tr>
<td style="text-align: left;">Verification mappings</td>
<td style="text-align: right;">29</td>
</tr>
<tr>
<td style="text-align: left;">Security scan scripts</td>
<td style="text-align: right;">14</td>
</tr>
<tr>
<td style="text-align: left;">Test scripts</td>
<td style="text-align: right;">14</td>
</tr>
<tr>
<td style="text-align: left;">NIST controls automated</td>
<td style="text-align: right;">14</td>
</tr>
<tr>
<td style="text-align: left;">GitHub issues (audit trail)</td>
<td style="text-align: right;">170+</td>
</tr>
</tbody>
</table>
</div>
<p>The daily commit rate averaged 24.7 commits/day, with the Security
Verification Toolkit alone accounting for 463 commits, 94 version tags,
and 26,630 lines of code. These figures are not presented as
benchmarks—the commit rate reflects a development style characterized by
frequent, atomic commits rather than large batch changes—but they
indicate the throughput achievable when AI agents handle drafting and
the engineer focuses on review and direction.</p>
<p>Figure <a href="#fig:code-churn" data-reference-type="ref"
data-reference="fig:code-churn">4</a> shows the daily code churn across
all repositories. Every period shows net-positive creation (green
additions exceeding red deletions), with a sustained deletion rate
indicating active refactoring rather than write-once code. The
48,000-line addition spike (late January) corresponds to the Security
Verification Toolkit’s test suite expansion—a compliance-critical
investment that the AI agent produced concurrently with the scanning
scripts.</p>
<figure id="fig:code-churn" data-latex-placement="htbp">
<embed src="visualizations/code_churn.pdf" />
<figcaption>Code churn across all repositories. Green bars show lines
added; red bars show lines deleted. Net-positive creation in every
period, with sustained deletion indicating active
refactoring.</figcaption>
</figure>
<p>More significant than the raw volume is the <em>ratio of compliance
artifacts to implementation code</em>. Traditional compliance workflows
produce documentation as a separate, sequential activity after
implementation. In the AI-assisted workflow, compliance artifacts
(requirements, verification matrices, decision memos, traceability
mappings, attestation PDFs) are generated <em>concurrently</em> with
implementation as a natural byproduct of the agent interaction. The
7 decision memoranda, 29 requirements, and 29 verification mappings in
SendCUIEmail were produced in the same sessions that generated the
implementation code, not in a separate documentation sprint.</p>
<h2 id="the-review-centric-workflow">The Review-Centric Workflow</h2>
<p>The most significant shift introduced by AI-assisted compliance
development is the transition from an <em>authoring</em> model to a
<em>review</em> model. In traditional compliance work, an engineer reads
the governing standard, interprets its requirements, drafts the
compliance artifact, and submits it for review. With AI assistance, the
engineer specifies the standard and reviews the AI-generated artifact
for accuracy.</p>
<p>This shift has two implications. First, it is faster: reviewing a
draft is consistently less effort than producing one from scratch.
Second, it changes the <em>skill profile</em> required: the engineer
must be a competent reviewer of compliance documents rather than a
competent author. This is a meaningful distinction—many engineers who
understand the technical standards struggle with the formal writing
conventions of government documentation.</p>
<h2 id="concurrent-multi-project-scalability">Concurrent Multi-Project
Scalability</h2>
<p>The review-centric workflow has a second-order implication: because
the human’s role shifts from author to reviewer, a single engineer can
oversee multiple AI-assisted projects concurrently. During the
development of this paper, the author maintained five active projects
simultaneously—SendCUIEmail (CUI encryption), a decision documentation
system, a Security Verification Toolkit, this white paper, and a
Scrum-based agent orchestration system—each with its own AI agent
sessions and compliance artifacts. These five projects span seven git
repositories tracked in this paper’s visualization data, with additional
supporting repositories (agent templates, process framework) bringing
the total to 16.</p>
<p>Critically, these projects are not merely concurrent; they
<em>cross-pollinate</em>. Patterns discovered in one project feed into
others: the Security Verification Toolkit’s scanning infrastructure was
applied to the white paper repository (Section <a
href="#sec:casestudy-toolkit" data-reference-type="ref"
data-reference="sec:casestudy-toolkit">6</a>); the SendCUIEmail
project’s agent conventions (<code>AGENTS.md</code>) informed the
multi-agent architecture described in Section <a href="#sec:agents"
data-reference-type="ref" data-reference="sec:agents">7</a>; the Scrum
repo’s team structure informed Section <a href="#sec:scrum"
data-reference-type="ref" data-reference="sec:scrum">7.4</a>; and this
paper documents the methodology used across all projects, creating a
feedback loop that improves each project’s compliance posture.</p>
<p>Figure <a href="#fig:cumulative-commits" data-reference-type="ref"
data-reference="fig:cumulative-commits">5</a> illustrates this
concurrent development pattern: the cumulative commit timeline shows
multiple repositories advancing simultaneously, with the Security
Verification Toolkit exhibiting the steepest growth curve while other
projects progress in parallel bursts.</p>
<p>This cross-project learning is facilitated by the
<code>CLAUDE.md</code> convention: insights captured in one project’s
instructions propagate to others when the engineer applies the same
patterns (semantic versioning, interaction logging, QA standards) across
repositories. The human engineer serves as the integrator—reviewing
agent output across projects, recognizing transferable patterns, and
directing agents to apply lessons learned from one domain to another.
This is a scalability model that would be impractical without AI
assistance: the documentation and compliance overhead of five
simultaneous government projects would overwhelm a single engineer
working manually.</p>
<p>Moreover, the methodology itself is refined iteratively as the
projects progress. The interaction logging requirement (Section <a
href="#sec:methodology-git" data-reference-type="ref"
data-reference="sec:methodology-git">3.5</a>) did not exist at project
inception—it was added mid-session when the engineer recognized the need
for audit traceability. Similarly, semantic versioning, the
<code>build.sh</code> script, and the Scrum-based orchestration model
were all incorporated as the engineer observed gaps during active
development. This organic refinement is itself documented in the GitHub
issue trail, creating a meta-level record of how the compliance process
evolved. The ability to refine tooling and process <em>while
simultaneously producing compliant artifacts</em> is a distinctive
advantage of the AI-assisted approach: the agent can update its own
instructions, rebuild its infrastructure, and continue producing
deliverables without the context-switching penalty that a human author
would incur.</p>
<figure id="fig:cumulative-commits" data-latex-placement="htbp">
<embed src="visualizations/cumulative_commits.pdf" />
<figcaption>Cumulative commits across all seven repositories over the
four-week development period. The Security Verification Toolkit (green,
dotted) dominates with 463 commits and exhibits the steepest growth
curve. Multiple repositories advance concurrently, demonstrating the
scalability of the AI-assisted review-centric workflow.</figcaption>
</figure>
<p>The scope of this concurrent work extends beyond software compliance.
The author’s AI-assisted workflow originated with a CAD-based house
construction project and a speech-processing application before evolving
into the government compliance domain examined here. Across 16 active
repositories—spanning systems engineering, hardware interfaces, music
production, web development, and federal information security—the same
patterns apply: AI agents draft artifacts, the human reviews and
directs, and the process is documented through git and issue tracking.
The methodology is not specific to government compliance; it is a
general-purpose approach to engineering documentation that happens to
map well to federal requirements.</p>
<p>A key enabler of cross-project learning is <em>agent instruction
ingestion</em>: AI agents read instruction files
(<code>CLAUDE.md</code>, <code>AGENTS.md</code>,
<code>agents.json</code>) from other repositories, absorbing patterns,
conventions, and compliance requirements established in sibling
projects. This naturally led to the creation of a canonical agent
templates repository (<a
href="https://github.com/brucedombrowski/ai-agents"
class="uri">https://github.com/brucedombrowski/ai-agents</a>) containing
model-agnostic role definitions that any project can inherit. The
templates separate the <em>what</em> (role responsibilities, standards,
interaction protocols) from the <em>how</em> (vendor-specific model
selection and tool configuration), allowing the same compliance agent
patterns to be implemented across different AI platforms. This
repository itself emerged organically from the white paper development
process—an example of the methodology producing reusable infrastructure
as a byproduct of compliance work.</p>
<h2
id="stakeholder-accessibility-bridging-the-cli-browser-gap">Stakeholder
Accessibility: Bridging the CLI-Browser Gap</h2>
<p>The methodology described in this paper is CLI-first: the engineer
works in AI coding agents, git, and shell scripts. This creates an
adoption barrier when the goal is team-wide participation by
non-technical stakeholders—program managers, team leads, auditors—who
will not install command-line tools.</p>
<p>The key insight is that while the <em>engineering</em> happens in the
CLI, the <em>outputs</em> are entirely browser-accessible. GitHub and
GitLab web interfaces render commit history, file diffs (green lines for
additions, red for deletions), issue threads, and merge request
discussions without requiring any software installation. The
stakeholder’s workflow reduces to: open a URL, review the diff, leave a
comment, click approve. This is a one-page desk instruction, not a
training program.</p>
<p>This pattern was validated in practice: a team lead adopted GitLab
for versioning periodic database exports to CSV, adding configuration
management (NIST SP 800-53 CM-3) to previously untracked operational
data. The team lead did not learn git—they learned to click “History”
and read a diff. The CSV format is critical: unlike binary formats
(Excel <code>.xlsx</code>, PDF), CSV files produce human-readable
line-by-line diffs in the browser. For teams working with Excel files, a
pre-commit hook that auto-converts <code>.xlsx</code> to
<code>.csv</code> provides the same visibility without changing the
user’s workflow.</p>
<p>When whole-team review is required, the branch-and-merge-request
workflow provides structured approval entirely within the browser.
Branch protection rules enforce that (1) all changes to the main branch
must go through a merge request, (2) required reviewers must approve
before merge, and (3) the author cannot approve their own changes. These
guardrails map directly to NIST SP 800-53 CM-3 (change control), AC-5
(separation of duties), and AU-3 (audit trail). Once configured, they
are enforced automatically—the merge button is physically disabled until
all conditions are met.</p>
<p>The generated visualizations (Section <a href="#sec:visualization"
data-reference-type="ref" data-reference="sec:visualization">8.6</a>)
serve a similar accessibility function. A chart showing 642 commits
across 7 repositories in under four weeks communicates project scope
more effectively to a non-technical audience than any paragraph. The
animated tree visualization (gource) showing file creation and
modification over time has proven particularly effective for conveying
the scale and structure of development activity to stakeholders
unfamiliar with version control concepts.</p>
<h2 id="sec:visualization">Git Data Visualization</h2>
<p>To support both the research objectives of this paper and the
practical need to communicate project status to non-technical
stakeholders, we developed a visualization pipeline that extracts data
from git repositories and produces publication-quality charts.</p>
<p>The pipeline follows the sequence: <code>git log</code> (structured
data extraction) <span class="math inline">\(\rightarrow\)</span> pandas
(aggregation and analysis) <span
class="math inline">\(\rightarrow\)</span> matplotlib with SciencePlots
styling (IEEE-formatted charts) <span
class="math inline">\(\rightarrow\)</span> matplot2tikz (PGFPlots export
for native LaTeX inclusion). Each chart is generated in three formats:
PNG (300 DPI, for presentations), PDF (vector, for print), and TikZ
(<code>.tex</code>, for direct <code>\input{}</code> into LaTeX
documents).</p>
<p>The toolkit comprises both custom analysis scripts and established
open-source tools:</p>
<ul>
<li><p><strong>onefetch</strong>: Repository summary cards showing
languages, lines of code, commits, and version tags per repo.</p></li>
<li><p><strong>git-of-theseus</strong>: Code survival analysis using
Kaplan-Meier methods—cohort stack plots showing how code ages over time,
and extension/directory breakdowns showing how the codebase structure
evolves.</p></li>
<li><p><strong>gource</strong>: Animated tree visualization rendering
repository history as a growing organism, with files as nodes and
contributors as actors.</p></li>
<li><p><strong>Custom cross-repo analysis</strong>: Cumulative commit
timelines, daily activity by repository, code churn (additions vs.
deletions), commit pattern analysis (hour of day, day of week), and
ecosystem timeline (Gantt-style active development windows).</p></li>
</ul>
<p>Applied to the seven repositories in this ecosystem (totaling 642
commits and 34,000+ lines of code over four weeks), the visualizations
revealed several patterns. Figure <a href="#fig:repo-comparison"
data-reference-type="ref" data-reference="fig:repo-comparison">6</a>
shows the ecosystem overview: the Security Verification Toolkit
dominates with 463 commits, 26,630 lines of code, and 94 version tags.
Figure <a href="#fig:ecosystem-timeline" data-reference-type="ref"
data-reference="fig:ecosystem-timeline">7</a> shows the active
development windows—multiple repositories developed concurrently by a
single engineer with AI agent support. Additional findings include:
<code>scripts/</code> and <code>tests/</code> directories grew in
lockstep (indicating disciplined test coverage); development activity
concentrated on weekdays with near-zero weekend commits; and the code
cohort analysis confirmed that all code is 2026-vintage—consistent with
a rapidly growing project where code survival analysis is not yet
meaningful but growth trajectories are clearly visible.</p>
<figure id="fig:repo-comparison" data-latex-placement="htbp">
<embed src="visualizations/repo_comparison.pdf" />
<figcaption>Repository ecosystem overview. Left: total commits per
repository. Center: lines of code. Right: version tags (releases). The
Security Verification Toolkit dominates all three metrics, reflecting
its maturity as the most actively developed case study.</figcaption>
</figure>
<figure id="fig:ecosystem-timeline" data-latex-placement="htbp">
<embed src="visualizations/ecosystem_timeline.pdf" />
<figcaption>Active development windows for each repository. Bar length
indicates the period between first and last commit; labels show total
commit counts. Multiple repositories were developed concurrently by a
single engineer using AI agent assistance.</figcaption>
</figure>
<p>Figure <a href="#fig:commit-patterns" data-reference-type="ref"
data-reference="fig:commit-patterns">8</a> shows the temporal
distribution of commits: peak activity occurs at 17:00 UTC (noon
Eastern) and 03:00–04:00 UTC (late night), with near-zero weekend
commits. This pattern—high weekday intensity with no weekend work—is
characteristic of a sustainable AI-assisted development cadence where
the engineer directs intensive sessions during focused work hours rather
than spreading effort thinly across calendar time.</p>
<figure id="fig:commit-patterns" data-latex-placement="htbp">
<embed src="visualizations/commit_patterns.pdf" />
<figcaption>Development patterns across the ecosystem. Left: commits by
hour of day (UTC). Right: commits by day of week (blue = weekday, orange
= weekend). Peak activity at 17:00 UTC and 03:00–04:00 UTC; near-zero
weekend commits.</figcaption>
</figure>
<p>These visualizations serve dual purpose: they are research artifacts
that quantify the development activity described in this paper, and they
are communication tools that make the same data accessible to
non-technical reviewers through browser-viewable charts and an animated
video. A training slide deck (<code>git-workflow-training.pptx</code>)
and a desk instruction (<code>DI-GIT-001</code>) were produced as
companion artifacts to support organizational adoption.</p>
<h2 id="human-in-the-loop-compliance">Human-in-the-Loop Compliance</h2>
<p>Government frameworks increasingly require evidence of human
oversight in automated processes. Agentic AI tools with explicit
permission models—where each file write, command execution, and code
edit requires developer approval—provide natural evidence of
human-in-the-loop oversight. Every action taken by the agent is logged
and approved, creating an audit trail that maps to the “authorized use”
requirements common in government security frameworks.</p>
<p>The <code>CLAUDE.md</code> convention further supports compliance by
encoding organizational and project-specific constraints that persist
across sessions. An organization’s compliance officer could define
<code>CLAUDE.md</code> templates that encode mandatory requirements,
ensuring that all AI-assisted development within the organization
operates within approved boundaries.</p>
<h2 id="standards-based-review-process">Standards-Based Review
Process</h2>
<p>The review agent itself operates according to established QA
standards, making the review process auditable and reproducible.
Table <a href="#tab:qa-standards" data-reference-type="ref"
data-reference="tab:qa-standards">[tab:qa-standards]</a> maps each
aspect of the review process to its governing standard.</p>
<div class="table*">
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Standard</strong></th>
<th style="text-align: left;"><strong>Control</strong></th>
<th style="text-align: left;"><strong>Application</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">IEEE 1028 <span class="citation"
data-cites="ieee1028">(IEEE Computer Society 2008)</span></td>
<td style="text-align: left;">Software Reviews</td>
<td style="text-align: left;">Review structure: severity classification
(CRITICAL/MINOR), findings format, disposition</td>
</tr>
<tr>
<td style="text-align: left;">IEEE 29148 <span class="citation"
data-cites="ieee29148">(ISO/IEC/IEEE 2018)</span></td>
<td style="text-align: left;">Requirements Engineering</td>
<td style="text-align: left;">Traceability verification: standard <span
class="math inline">\(\rightarrow\)</span> requirement <span
class="math inline">\(\rightarrow\)</span> implementation <span
class="math inline">\(\rightarrow\)</span> test</td>
</tr>
<tr>
<td style="text-align: left;">NIST SP 800-53 <span class="citation"
data-cites="nist80053">(Force 2020)</span></td>
<td style="text-align: left;">AC-5: Separation of Duties</td>
<td style="text-align: left;">Review agent denied write/edit tools;
auditors cannot modify what they audit</td>
</tr>
<tr>
<td style="text-align: left;">NIST SP 800-53 <span class="citation"
data-cites="nist80053">(Force 2020)</span></td>
<td style="text-align: left;">SA-11: Developer Testing</td>
<td style="text-align: left;">Claims verified against source files;
assertions checked against implementation</td>
</tr>
<tr>
<td style="text-align: left;">ISO/IEC 25010 <span class="citation"
data-cites="iso25010">(ISO/IEC 2011)</span></td>
<td style="text-align: left;">Software Quality</td>
<td style="text-align: left;">Documentation quality: completeness,
accuracy, consistency checks</td>
</tr>
<tr>
<td style="text-align: left;">MIL-STD-498 <span class="citation"
data-cites="milstd498">(Department of Defense 1994)</span></td>
<td style="text-align: left;">A.5.19: Traceability</td>
<td style="text-align: left;">Cross-reference integrity between REQ,
VER, DM, and source code</td>
</tr>
</tbody>
</table>
</div>
<p>This standards-based approach ensures that the review process itself
can withstand audit scrutiny—a critical consideration for government
programs where the QA methodology must be as defensible as the artifacts
it evaluates. All review findings are documented as GitHub issues with
structured severity, recommendation, and standard-violated fields,
providing a traceable audit record per IEEE 1028.</p>
<h2 id="limitations">Limitations</h2>
<p>Several limitations should be noted:</p>
<ol>
<li><p><strong>Model knowledge currency</strong>: LLM training data has
a cutoff date, meaning recent revisions to standards (e.g., updates to
NIST SP 800-171 Rev. 3, or the transition from FIPS 140-2 to FIPS 140-3
for new CMVP submissions since 2021) may not be reflected. Developers
must verify that AI-cited standards are current.</p></li>
<li><p><strong>No formal verification</strong>: AI-generated compliance
claims are assertions, not proofs. They do not substitute for formal
testing, independent audit, or certification processes such as CMVP
validation.</p></li>
<li><p><strong>Organizational specificity</strong>: Government
compliance is highly context-dependent. The same standard may be
interpreted differently across agencies, and AI agents lack
organizational knowledge without explicit instruction.</p></li>
<li><p><strong>Classification boundaries</strong>: AI tools operating in
cloud-connected modes are unsuitable for classified work. The
methodology presented here applies only to unclassified and CUI
environments.</p></li>
<li><p><strong>Case study maturity</strong>: All three case study
projects are under active development and have not reached production
release status (SendCUIEmail v0.17.3, Decisions v0.4, Security Toolkit
v2.7.3). The compliance artifacts and methodology demonstrated here
reflect a development-phase workflow; production deployment would
require additional validation, independent testing, and formal
authority-to-operate processes.</p></li>
</ol>
<h2 id="reproducibility-and-process-documentation">Reproducibility and
Process Documentation</h2>
<p>This paper itself was produced using the methodology it describes.
The white paper repository maintains a two-tier documentation structure:
<code>PROCESS.md</code> provides a human-readable executive summary of
each development session, while GitHub issues serve as the
authoritative, machine-queryable record of all human-agent
interactions.</p>
<p>As of this writing, the repository contains 33 GitHub issues spanning
11 development sessions, with each issue labeled according to the scheme
described in Section <a href="#sec:methodology-git"
data-reference-type="ref" data-reference="sec:methodology-git">3.5</a>.
The git history contains 39 semantically versioned commits across 7
release tags (v0.1.0 through v0.7.0), each corresponding to a distinct
compliance-relevant action. The repository also contains a visualization
toolkit that generates 10 publication-quality charts from git data
across the ecosystem, of which 6 are included as figures in this paper
(Figures <a href="#fig:repo-comparison" data-reference-type="ref"
data-reference="fig:repo-comparison">6</a>–<a href="#fig:code-churn"
data-reference-type="ref" data-reference="fig:code-churn">4</a>).
Together, these records provide sufficient information for an
independent team to reproduce the development process or for an auditor
to verify that every artifact has a documented provenance chain.</p>
<p>This dual-track approach—git for configuration management, GitHub
issues for interaction traceability—mirrors the separation between
configuration management (NIST SP 800-53 CM-3) and audit logging (NIST
SP 800-53 AU-3) that government frameworks prescribe. The combination
ensures that the process is documented at both the artifact level (what
changed) and the decision level (why it changed).</p>
<h1 id="sec:future">Future Work</h1>
<p>Several directions merit further investigation:</p>
<ol>
<li><p><strong>Automated compliance testing</strong>: Integrating AI
agents with continuous integration pipelines to validate compliance
assertions against code changes.</p></li>
<li><p><strong>Standard-specific agents</strong>: Training or
fine-tuning agents on specific government standards (e.g., a NIST SP
800-171 specialist agent) to improve requirement extraction
accuracy.</p></li>
<li><p><strong>Cross-reference validation</strong>: Building tools that
automatically verify citations between compliance artifacts
(requirements <span class="math inline">\(\leftrightarrow\)</span>
verification <span class="math inline">\(\leftrightarrow\)</span>
code).</p></li>
<li><p><strong>FedRAMP and CMMC application</strong>: Extending the
methodology to broader compliance frameworks such as FedRAMP
authorization packages and CMMC assessments.</p></li>
<li><p><strong>Comparative studies</strong>: Quantitative comparison of
AI-assisted vs. manual compliance documentation effort across multiple
projects and team sizes.</p></li>
<li><p><strong>Cross-platform validation</strong>: While the methodology
is designed to be platform-agnostic, our case studies use a single AI
agent implementation. Replicating the workflow with alternative agentic
tools would empirically validate portability and identify which
properties are tool-dependent vs. methodology-dependent.</p></li>
</ol>
<h1 id="sec:conclusion">Conclusion</h1>
<p>This paper has demonstrated a five-phase methodology for combining
git version control and AI coding agents to address the challenge of
building software that meets government compliance requirements. Through
three case studies, we showed that AI agents can produce structurally
sound compliance artifacts including requirements specifications,
decision memoranda, verification documents, and automated compliance
attestations, while the interactive approval model provides the human
oversight that government frameworks require.</p>
<p>The key insight is not that AI replaces compliance expertise, but
that it <em>restructures</em> the compliance workflow. The engineer’s
role shifts from author to reviewer, the documentation burden decreases
without sacrificing rigor, and the multi-agent architecture enables
scalable compliance workflows for projects of varying complexity.
Critically, the version control and interaction traceability layer—git
for configuration management, GitHub issues for human-agent interaction
logging—provides the audit evidence that government frameworks demand,
mapping directly to NIST SP 800-53 controls CM-3 and AU-3.</p>
<p>This paper itself demonstrates the methodology: it was produced
across multiple AI agent sessions, with every human directive and agent
action logged as GitHub issues, every change captured in semantically
versioned git commits, and the entire process reproducible from the
public repository. The fact that an AI agent can assist in producing
both the compliance artifacts <em>and</em> the auditable process
documentation for those artifacts suggests a path toward significantly
reducing the overhead of government compliance work.</p>
<p>This work is not a proof of concept. The methodology, agent
configurations, and process artifacts presented here are in active use
across 16 repositories spanning government compliance, systems
engineering, security tooling, and CAD—real projects with real
deliverables. The approach produces measurable outcomes: 642 commits,
34,000+ lines of code, 136 release tags, and over 130 compliance
artifacts across seven repositories in 26 calendar days—produced by a
single engineer with AI agent support. This demonstrates more consistent
documentation (fewer gaps, stronger traceability), faster delivery (the
review-centric workflow eliminates the authoring bottleneck), and
reduced personnel requirements (one engineer with AI agents sustains the
documentation overhead that traditionally demands a dedicated compliance
team). The engineering experience itself improves when the tedious parts
of compliance work are handled by agents and the human focuses on
judgment, direction, and review. As government agencies and contractors
face increasing pressure to demonstrate compliance across expanding
regulatory frameworks, the methodology presented here offers a
practical, field-tested foundation for getting real work done.</p>
<h1 class="unnumbered" id="acknowledgments">Acknowledgments</h1>
<p>This paper and its supporting artifacts were developed using the
methodology it describes, with Claude Code (Anthropic, model: Claude
Opus) as the AI agent implementation. All source materials, including
the LaTeX source, agent configurations, git history, and process
documentation, are available at <a
href="https://github.com/brucedombrowski/WhitePaper"
class="uri">https://github.com/brucedombrowski/WhitePaper</a>.</p>
<div id="refs" class="references csl-bib-body hanging-indent"
role="list">
<div id="ref-claudecode" class="csl-entry" role="listitem">
Anthropic. 2025. <em><span class="nocase">Claude Code: Anthropic’s
AI-Powered Command-Line Development Tool</span></em>. Software. <a
href="https://docs.anthropic.com/en/docs/claude-code">https://docs.anthropic.com/en/docs/claude-code</a>.
</div>
<div id="ref-nist80090a" class="csl-entry" role="listitem">
Barker, Elaine, and John Kelsey. 2015. <em><span class="nocase">NIST SP
800-90A: Recommendation for Random Number Generation Using Deterministic
Random Bit Generators</span></em>. Special Publication 800-90A Rev. 1.
National Institute of Standards; Technology. <a
href="https://csrc.nist.gov/publications/detail/sp/800-90a/rev-1/final">https://csrc.nist.gov/publications/detail/sp/800-90a/rev-1/final</a>.
</div>
<div id="ref-rfc2119" class="csl-entry" role="listitem">
Bradner, Scott. 1997. <em><span class="nocase">RFC 2119: Key Words for
Use in RFCs to Indicate Requirement Levels</span></em>. Internet
Engineering Task Force. <a
href="https://www.rfc-editor.org/rfc/rfc2119">https://www.rfc-editor.org/rfc/rfc2119</a>.
</div>
<div id="ref-bod2201" class="csl-entry" role="listitem">
Cybersecurity and Infrastructure Security Agency. 2021. <em><span
class="nocase">BOD 22-01: Reducing the Significant Risk of Known
Exploited Vulnerabilities</span></em>. Binding Operational Directive. <a
href="https://www.cisa.gov/known-exploited-vulnerabilities-catalog">https://www.cisa.gov/known-exploited-vulnerabilities-catalog</a>.
</div>
<div id="ref-milstd498" class="csl-entry" role="listitem">
Department of Defense. 1994. <em><span class="nocase">MIL-STD-498:
Software Development and Documentation</span></em>. Military Standard
MIL-STD-498. U.S. Department of Defense.
</div>
<div id="ref-securitytoolkit" class="csl-entry" role="listitem">
Dombrowski, Bruce. 2026a. <em><span>Security Verification Toolkit:
Automated NIST SP 800-53 Control Verification</span></em>. GitHub
repository. <a
href="https://github.com/brucedombrowski/security-toolkit">https://github.com/brucedombrowski/security-toolkit</a>.
</div>
<div id="ref-sendcuiemail" class="csl-entry" role="listitem">
Dombrowski, Bruce. 2026b. <em><span>SendCUIEmail: CUI Email Encryption
Tool</span></em>. GitHub repository. <a
href="https://github.com/brucedombrowski/SendCUIEmail">https://github.com/brucedombrowski/SendCUIEmail</a>.
</div>
<div id="ref-nist80038a" class="csl-entry" role="listitem">
Dworkin, Morris. 2001. <em><span class="nocase">NIST SP 800-38A:
Recommendation for Block Cipher Modes of Operation</span></em>. Special
Publication Nos. 800-38A. National Institute of Standards; Technology.
<a
href="https://csrc.nist.gov/publications/detail/sp/800-38a/final">https://csrc.nist.gov/publications/detail/sp/800-38a/final</a>.
</div>
<div id="ref-fan2023llmse" class="csl-entry" role="listitem">
Fan, Angela, Beliz Gokkaya, Mark Harman, et al. 2023. <span>“Large
Language Models for Software Engineering: Survey and Open
Problems.”</span> <em>arXiv Preprint arXiv:2310.03533</em>.
</div>
<div id="ref-nist80053" class="csl-entry" role="listitem">
Force, Joint Task. 2020. <em><span class="nocase">NIST SP 800-53:
Security and Privacy Controls for Information Systems and
Organizations</span></em>. Special Publication 800-53 Rev. 5. National
Institute of Standards; Technology. <a
href="https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final">https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final</a>.
</div>
<div id="ref-nist80063b" class="csl-entry" role="listitem">
Grassi, Paul A., James L. Fenton, Elaine M. Newton, et al. 2017.
<em><span class="nocase">NIST SP 800-63B: Digital Identity Guidelines —
Authentication and Lifecycle Management</span></em>. Special Publication
Nos. 800-63B. National Institute of Standards; Technology. <a
href="https://csrc.nist.gov/publications/detail/sp/800-63b/final">https://csrc.nist.gov/publications/detail/sp/800-63b/final</a>.
</div>
<div id="ref-ieee1028" class="csl-entry" role="listitem">
IEEE Computer Society. 2008. <em><span class="nocase">IEEE 1028:
Standard for Software Reviews and Audits</span></em>. IEEE Standard Nos.
1028-2008. Institute of Electrical; Electronics Engineers.
</div>
<div id="ref-32cfr2002" class="csl-entry" role="listitem">
Information Security Oversight Office. 2016. <em><span>32 CFR Part 2002:
Controlled Unclassified Information</span></em>. <a
href="https://www.ecfr.gov/current/title-32/subtitle-B/chapter-XX/part-2002">https://www.ecfr.gov/current/title-32/subtitle-B/chapter-XX/part-2002</a>.
</div>
<div id="ref-iso25010" class="csl-entry" role="listitem">
ISO/IEC. 2011. <em><span class="nocase">ISO/IEC 25010: Systems and
Software Quality Requirements and Evaluation (SQuaRE)</span></em>.
International Standard 25010:2011. International Organization for
Standardization.
</div>
<div id="ref-ieee29148" class="csl-entry" role="listitem">
ISO/IEC/IEEE. 2018. <em><span class="nocase">ISO/IEC/IEEE 29148: Systems
and Software Engineering — Life Cycle Processes — Requirements
Engineering</span></em>. International Standard 29148:2018. IEEE.
</div>
<div id="ref-khan2022apidoc" class="csl-entry" role="listitem">
Khan, Junaed Younus, and Gias Uddin. 2022. <span>“Automatic Generation
of API Documentation.”</span> <em>Proceedings of the 30th IEEE/ACM
International Conference on Program Comprehension</em>, 497–501.
</div>
<div id="ref-nist80088" class="csl-entry" role="listitem">
Kissel, Richard, Andrew Regenscheid, Matthew Scholl, and Kevin Stine.
2014. <em><span class="nocase">NIST SP 800-88: Guidelines for Media
Sanitization</span></em>. Special Publication 800-88 Rev. 1. National
Institute of Standards; Technology. <a
href="https://csrc.nist.gov/publications/detail/sp/800-88/rev-1/final">https://csrc.nist.gov/publications/detail/sp/800-88/rev-1/final</a>.
</div>
<div id="ref-fips1402" class="csl-entry" role="listitem">
National Institute of Standards and Technology. 2001a. <em><span
class="nocase">FIPS 140-2: Security Requirements for Cryptographic
Modules</span></em>. NIST. <a
href="https://csrc.nist.gov/publications/detail/fips/140/2/final">https://csrc.nist.gov/publications/detail/fips/140/2/final</a>.
</div>
<div id="ref-fips197" class="csl-entry" role="listitem">
National Institute of Standards and Technology. 2001b. <em><span>FIPS
197: Advanced Encryption Standard (AES)</span></em>. NIST. <a
href="https://csrc.nist.gov/publications/detail/fips/197/final">https://csrc.nist.gov/publications/detail/fips/197/final</a>.
</div>
<div id="ref-fips199" class="csl-entry" role="listitem">
National Institute of Standards and Technology. 2004. <em><span
class="nocase">FIPS 199: Standards for Security Categorization of
Federal Information and Information Systems</span></em>. NIST. <a
href="https://csrc.nist.gov/publications/detail/fips/199/final">https://csrc.nist.gov/publications/detail/fips/199/final</a>.
</div>
<div id="ref-nist800171" class="csl-entry" role="listitem">
Ross, Ron, Victoria Pillitteri, Kelley Dempsey, Mark Riddle, and Gary
Guissanie. 2020. <em><span class="nocase">NIST SP 800-171: Protecting
Controlled Unclassified Information in Nonfederal Systems and
Organizations</span></em>. Special Publication 800-171 Rev. 2. National
Institute of Standards; Technology. <a
href="https://csrc.nist.gov/publications/detail/sp/800-171/rev-2/final">https://csrc.nist.gov/publications/detail/sp/800-171/rev-2/final</a>.
</div>
<div id="ref-scrumguide" class="csl-entry" role="listitem">
Schwaber, Ken, and Jeff Sutherland. 2020. <em><span>The Scrum
Guide</span></em>. Scrum.org. <a
href="https://scrumguides.org/scrum-guide.html">https://scrumguides.org/scrum-guide.html</a>.
</div>
<div id="ref-nist800132" class="csl-entry" role="listitem">
Turan, Meltem Sönmez, Elaine Barker, William Burr, and Lily Chen. 2010.
<em><span class="nocase">NIST SP 800-132: Recommendation for
Password-Based Key Derivation</span></em>. Special Publication Nos.
800-132. National Institute of Standards; Technology. <a
href="https://csrc.nist.gov/publications/detail/sp/800-132/final">https://csrc.nist.gov/publications/detail/sp/800-132/final</a>.
</div>
</div>
</body>
</html>
